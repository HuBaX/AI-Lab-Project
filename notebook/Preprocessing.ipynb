{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from datasets import Dataset as HuggingFaceDataset\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "data_path = \"data\\\\train\"\n",
    "dataset_files = []\n",
    "word_counts = Counter()\n",
    "\n",
    "# Get all HuggingFace Datasets\n",
    "for listed_file in os.listdir(data_path):\n",
    "    if listed_file.endswith(\".arrow\"):\n",
    "        dataset_files.append(listed_file)\n",
    "\n",
    "# Get the number of rows in each file and calculate the total number of rows\n",
    "for file in dataset_files:\n",
    "    ds = HuggingFaceDataset.from_file(f\"{data_path}\\\\{file}\")\n",
    "    # Das zählt nur die Buchstaben und deren Häufigkeit\n",
    "    for sentence in ds[\"prompt\"]:\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            if token.is_alpha:\n",
    "                word_counts[token.text] = word_counts.get(token.text, 0) + 1\n",
    "\n",
    "vocab = [word for word, count in word_counts.most_common(10000)]\n",
    "vocab_size = len(vocab) + 3  # Increment vocab_size by 3 for <UNK>, <SOS> and <EOS> tags\n",
    "\n",
    "word2index = {word: i+3 for i, word in enumerate(vocab)}  # Shift indices by 3 for <UNK> and <EOS>\n",
    "word2index[\"<UNK>\"] = 0\n",
    "word2index[\"<SOS>\"] = 1\n",
    "word2index[\"<EOS>\"] = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Tokenization & Image Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [tok.text for tok in nlp(text)]\n",
    "\n",
    "def tokens_to_indices(tokens):\n",
    "    return [word2index[\"<SOS>\"]] + [word2index.get(word, 0) for word in tokens] + [word2index[\"<EOS>\"]]\n",
    "\n",
    "def tokenize_and_index_prompts(prompts):\n",
    "    return prompts.apply(tokenize).apply(tokens_to_indices).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_images(images):\n",
    "    transform = transforms.Compose([transforms.Resize((512,512)),\n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "    # Convert the PIL image to Torch tensor of size 512x512\n",
    "    return images.apply(transform).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_DIR = \"preprocessed\"\n",
    "os.makedirs(PREPROCESSED_DIR, exist_ok=True)  \n",
    "total_index = 0\n",
    "for i, file in enumerate(dataset_files):\n",
    "        ds = pd.DataFrame(HuggingFaceDataset.from_file(f\"{data_path}\\\\{file}\"))\n",
    "        \n",
    "        transformed_images = transform_images(ds[\"image\"])\n",
    "        tokenized_prompts = tokenize_and_index_prompts(ds[\"prompt\"])\n",
    "        for i in range(len(ds)):\n",
    "                np.save(f\"{PREPROCESSED_DIR}\\\\image_{total_index}.npy\", transformed_images[i])\n",
    "                np.save(f\"{PREPROCESSED_DIR}\\\\prompt_{total_index}.npy\", tokenized_prompts[i])\n",
    "                total_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word2index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2index, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
