{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a0941e17d962401f9514f3d87df0a65f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e311b0f9685146958444dcdc7870d099",
              "IPY_MODEL_8d77e6f4b38c41f0af41b07b0be3c107",
              "IPY_MODEL_fd3d00547fe54bcda39f22a6b18b7fc4"
            ],
            "layout": "IPY_MODEL_4475e962b705495195340471881ea91c"
          }
        },
        "e311b0f9685146958444dcdc7870d099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f31cea9b62204578a45548a972364780",
            "placeholder": "​",
            "style": "IPY_MODEL_039f546913984c7ca4de05d891370828",
            "value": "100%"
          }
        },
        "8d77e6f4b38c41f0af41b07b0be3c107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f75e2f36640c4154b7b2363a57143b9d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9eba77a7857a4310b4bfe37403d3736e",
            "value": 1
          }
        },
        "fd3d00547fe54bcda39f22a6b18b7fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79b58f471c4342feb83e107166be8651",
            "placeholder": "​",
            "style": "IPY_MODEL_5d42d7d8daac427db6a054141236e03d",
            "value": " 1/1 [00:00&lt;00:00, 38.47it/s]"
          }
        },
        "4475e962b705495195340471881ea91c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f31cea9b62204578a45548a972364780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "039f546913984c7ca4de05d891370828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f75e2f36640c4154b7b2363a57143b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eba77a7857a4310b4bfe37403d3736e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79b58f471c4342feb83e107166be8651": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d42d7d8daac427db6a054141236e03d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i323PgRmbOo4",
        "outputId": "80f04fe1-6b58-4644-fecd-a9fc657f37b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from random import seed\n",
        "from random import random\n",
        "import torchtext"
      ],
      "metadata": {
        "id": "Z3HcVEM4bTzn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "bt-M8MDXumpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86022df4-52ff-4219-f11d-a4bc3832f397"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diffusiondb = load_dataset('poloclub/diffusiondb', 'large_first_1k')\n",
        "\n",
        "train_df = pd.DataFrame(diffusiondb[\"train\"])\n",
        "train_df = train_df[[\"image\", \"prompt\"]]\n",
        "del diffusiondb\n",
        "train_df"
      ],
      "metadata": {
        "id": "sjwrT_ZqbXnZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488,
          "referenced_widgets": [
            "a0941e17d962401f9514f3d87df0a65f",
            "e311b0f9685146958444dcdc7870d099",
            "8d77e6f4b38c41f0af41b07b0be3c107",
            "fd3d00547fe54bcda39f22a6b18b7fc4",
            "4475e962b705495195340471881ea91c",
            "f31cea9b62204578a45548a972364780",
            "039f546913984c7ca4de05d891370828",
            "f75e2f36640c4154b7b2363a57143b9d",
            "9eba77a7857a4310b4bfe37403d3736e",
            "79b58f471c4342feb83e107166be8651",
            "5d42d7d8daac427db6a054141236e03d"
          ]
        },
        "outputId": "ec35e7b4-3edb-46f2-84b8-1b1f3ec05273"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset diffusiondb (/root/.cache/huggingface/datasets/poloclub___diffusiondb/large_first_1k/0.9.1/b3bc1e64570dc7149af62c4bac49ecfbce16b683dd4fee083292fae1afa95f7c)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0941e17d962401f9514f3d87df0a65f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 image  \\\n",
              "0    <PIL.WebPImagePlugin.WebPImageFile image mode=...   \n",
              "1    <PIL.WebPImagePlugin.WebPImageFile image mode=...   \n",
              "2    <PIL.WebPImagePlugin.WebPImageFile image mode=...   \n",
              "3    <PIL.WebPImagePlugin.WebPImageFile image mode=...   \n",
              "4    <PIL.WebPImagePlugin.WebPImageFile image mode=...   \n",
              "..                                                 ...   \n",
              "995  <PIL.WebPImagePlugin.WebPImageFile image mode=...   \n",
              "996  <PIL.WebPImagePlugin.WebPImageFile image mode=...   \n",
              "997  <PIL.WebPImagePlugin.WebPImageFile image mode=...   \n",
              "998  <PIL.WebPImagePlugin.WebPImageFile image mode=...   \n",
              "999  <PIL.WebPImagePlugin.WebPImageFile image mode=...   \n",
              "\n",
              "                                                prompt  \n",
              "0                   goddess portrait, ismail inceoglu   \n",
              "1                   goddess portrait, ismail inceoglu   \n",
              "2    portrait of king of candy mr harry haribo oil ...  \n",
              "3    super epic realistic nature photo trending on ...  \n",
              "4    super epic realistic nature photo trending on ...  \n",
              "..                                                 ...  \n",
              "995  portrait of haribo bear in future city, color ...  \n",
              "996  photo of terrifying witch, hyper detailed, flo...  \n",
              "997  portrait of haribo bear in future city, color ...  \n",
              "998  portrait of haribo bear in future city, color ...  \n",
              "999  portrait of haribo bear in future city, color ...  \n",
              "\n",
              "[1000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b140ba4-d8f7-47e8-9bee-a29412a2cc53\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>prompt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;PIL.WebPImagePlugin.WebPImageFile image mode=...</td>\n",
              "      <td>goddess portrait, ismail inceoglu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;PIL.WebPImagePlugin.WebPImageFile image mode=...</td>\n",
              "      <td>goddess portrait, ismail inceoglu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;PIL.WebPImagePlugin.WebPImageFile image mode=...</td>\n",
              "      <td>portrait of king of candy mr harry haribo oil ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;PIL.WebPImagePlugin.WebPImageFile image mode=...</td>\n",
              "      <td>super epic realistic nature photo trending on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;PIL.WebPImagePlugin.WebPImageFile image mode=...</td>\n",
              "      <td>super epic realistic nature photo trending on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>&lt;PIL.WebPImagePlugin.WebPImageFile image mode=...</td>\n",
              "      <td>portrait of haribo bear in future city, color ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>&lt;PIL.WebPImagePlugin.WebPImageFile image mode=...</td>\n",
              "      <td>photo of terrifying witch, hyper detailed, flo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>&lt;PIL.WebPImagePlugin.WebPImageFile image mode=...</td>\n",
              "      <td>portrait of haribo bear in future city, color ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>&lt;PIL.WebPImagePlugin.WebPImageFile image mode=...</td>\n",
              "      <td>portrait of haribo bear in future city, color ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>&lt;PIL.WebPImagePlugin.WebPImageFile image mode=...</td>\n",
              "      <td>portrait of haribo bear in future city, color ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b140ba4-d8f7-47e8-9bee-a29412a2cc53')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0b140ba4-d8f7-47e8-9bee-a29412a2cc53 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0b140ba4-d8f7-47e8-9bee-a29412a2cc53');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize(text):\n",
        "    return [tok.text for tok in nlp(text)]\n",
        "\n",
        "tokenize(\"Hallo ich bin Lukas\")"
      ],
      "metadata": {
        "id": "BI1kB4YSbdYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a894f558-4342-4543-cf87-49e575ad66d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hallo', 'ich', 'bin', 'Lukas']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = Counter()\n",
        "c = 0\n",
        "for sentence in train_df[\"prompt\"]:\n",
        "    doc = nlp(sentence)\n",
        "    \n",
        "    # Iterate over each token in the processed sentence\n",
        "    for token in doc:\n",
        "        # Check if the token is a word (excluding punctuation and whitespace)\n",
        "        if token.is_alpha:\n",
        "            # Increment the count for the word\n",
        "            word_counts[token.text] = word_counts.get(token.text, 0) + 1\n",
        "    \n",
        "    c += 1\n",
        "    if c > 20: \n",
        "      break\n",
        "\n",
        "# Print the word counts\n",
        "for word, count in word_counts.items():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "vWA1fbK5bh5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb7c363-5033-446e-a953-c999d571d23c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "goddess: 2\n",
            "portrait: 7\n",
            "ismail: 2\n",
            "inceoglu: 2\n",
            "of: 10\n",
            "king: 1\n",
            "candy: 1\n",
            "mr: 1\n",
            "harry: 1\n",
            "haribo: 1\n",
            "oil: 1\n",
            "painting: 1\n",
            "bloody: 1\n",
            "conquest: 1\n",
            "tap: 1\n",
            "e: 1\n",
            "super: 8\n",
            "epic: 12\n",
            "realistic: 8\n",
            "nature: 8\n",
            "photo: 8\n",
            "trending: 8\n",
            "on: 8\n",
            "instagram: 8\n",
            "with: 8\n",
            "lonely: 8\n",
            "person: 8\n",
            "in: 12\n",
            "yellow: 8\n",
            "raincoat: 8\n",
            "standing: 8\n",
            "at: 8\n",
            "a: 12\n",
            "distance: 8\n",
            "beautiful: 4\n",
            "princess: 4\n",
            "wearing: 4\n",
            "evil: 4\n",
            "black: 4\n",
            "oily: 4\n",
            "tar: 4\n",
            "by: 4\n",
            "hr: 4\n",
            "giger: 4\n",
            "greg: 4\n",
            "rutkowski: 4\n",
            "luis: 4\n",
            "royo: 4\n",
            "and: 8\n",
            "wayne: 4\n",
            "barlowe: 4\n",
            "k: 4\n",
            "mountains: 4\n",
            "lake: 4\n",
            "walley: 4\n",
            "liminal: 4\n",
            "emperor: 4\n",
            "palpatine: 4\n",
            "the: 4\n",
            "desert: 8\n",
            "tatooine: 4\n",
            "film: 4\n",
            "still: 4\n",
            "wide: 4\n",
            "shot: 4\n",
            "heat: 4\n",
            "sci: 4\n",
            "fi: 4\n",
            "dramatic: 4\n",
            "light: 4\n",
            "young: 2\n",
            "mark: 2\n",
            "hamill: 2\n",
            "as: 2\n",
            "child: 2\n",
            "star: 4\n",
            "wars: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class DiffusionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "        self.vocab_size, self.word2index = self.build_vocab()\n",
        "        self.transformed_images = self.transform_images()\n",
        "        self.tokenized_prompts = self.tokenize_and_index_prompts()\n",
        "        self.eos_index = self.word2index[\"<EOS>\"] \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.transformed_images[index]\n",
        "        prompt = self.tokenized_prompts[index]\n",
        "\n",
        "        #print(f\"Padded Prompt Shape: {padded_prompt.shape}\\nPadded Prompt: {padded_prompt}\\n\")\n",
        "        return image, torch.tensor(prompt)\n",
        "\n",
        "    def build_vocab(self):\n",
        "        word_counts = Counter()\n",
        "\n",
        "        # Das zählt nur die Buchstaben und deren Häufigkeit\n",
        "        #for tokens in self.data[\"prompt\"]:\n",
        "        #    word_counts.update(tokens)\n",
        "\n",
        "        for sentence in self.data[\"prompt\"]:\n",
        "          doc = nlp(sentence)\n",
        "          for token in doc:\n",
        "            if token.is_alpha:\n",
        "                word_counts[token.text] = word_counts.get(token.text, 0) + 1\n",
        "\n",
        "        vocab = [word for word, count in word_counts.most_common(5000)]\n",
        "        vocab_size = len(vocab) + 3  # Increment vocab_size by 2 for <UNK>, <SOS> and <EOS> tags\n",
        "\n",
        "        word2index = {word: i+3 for i, word in enumerate(vocab)}  # Shift indices by 3 for <UNK> and <EOS>\n",
        "        word2index[\"<UNK>\"] = 0\n",
        "        word2index[\"<SOS>\"] = 1\n",
        "        word2index[\"<EOS>\"] = 2\n",
        "\n",
        "        return vocab_size, word2index\n",
        "\n",
        "    def transform_images(self):\n",
        "      transform = transforms.Compose([transforms.Resize((256,256)),\n",
        "                                      transforms.ToTensor()])\n",
        "\n",
        "      # Convert the PIL image to Torch tensor of size 512x512\n",
        "      return self.data[\"image\"].apply(transform).to_list()\n",
        "\n",
        "\n",
        "\n",
        "    def tokenize_and_index_prompts(self):\n",
        "        return self.data[\"prompt\"].apply(self.tokenize).apply(self.tokens_to_indices).tolist()\n",
        "\n",
        "    def tokenize(self, text):\n",
        "      return [tok.text for tok in nlp(text)]\n",
        "\n",
        "    def tokens_to_indices(self, tokens):\n",
        "        return [self.word2index[\"<SOS>\"]] + [self.word2index.get(word, 0) for word in tokens] + [self.word2index[\"<EOS>\"]]"
      ],
      "metadata": {
        "id": "umn7FrIbbqeo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, n_layers, hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, hid_dim)\n",
        "\n",
        "        self.hidden_size = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "\n",
        "        batch_size = features.size(0)\n",
        "        hidden = features.unsqueeze(0).expand(self.n_layers, batch_size, self.hidden_size)\n",
        "        # Initialize the cell state with zeros\n",
        "        cell = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(features.device)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, n_layers, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden.contiguous(), cell.contiguous()))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "metadata": {
        "id": "mtuvvCgbbtQy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.jit import script_if_tracing\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "       \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = float(0.5)):\n",
        "\n",
        "        #trg = [batch size, trg len]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        #input = trg[0,:]\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            #output, hidden, cell = self.decoder(input, hidden.unsqueeze(0), cell.unsqueeze(0))\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token)\n",
        "\n",
        "            input = trg[:,t] if random() < teacher_forcing_ratio else top1\n",
        "            #input = trg[t] if teacher_force else top1.squeeze(0)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "C2GYGB9QkiTl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DiffusionDataset(train_df)\n",
        "del train_df"
      ],
      "metadata": {
        "id": "pShPemzekGS3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 512\n",
        "hidden_size = 256\n",
        "output_size = dataset.vocab_size\n",
        "n_layers = 2\n",
        "dec_dropout = 0.5\n",
        "\n",
        "batch_size = 2\n",
        "num_epochs = 10\n",
        "clip = 1\n",
        "\n",
        "# seed random number generator\n",
        "seed(1)"
      ],
      "metadata": {
        "id": "w-0WbtFHlEla"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.vocab_size"
      ],
      "metadata": {
        "id": "QkzZs6BmkH36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae949b1-226e-43b1-c765-57805c6b575e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1484"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "for img in dataset.transformed_images:\n",
        "  print(img.size())\n",
        "  c += 1\n",
        "  if c > 5:\n",
        "    break\n"
      ],
      "metadata": {
        "id": "bF8o7jtwkJdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bc60ba6-1467-417f-b044-fcbcc6a6894f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 256, 256])\n",
            "torch.Size([3, 256, 256])\n",
            "torch.Size([3, 256, 256])\n",
            "torch.Size([3, 256, 256])\n",
            "torch.Size([3, 256, 256])\n",
            "torch.Size([3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = EncoderCNN(embed_size, n_layers, hidden_size).to(device)\n",
        "decoder = DecoderRNN(output_size, embed_size, n_layers, hidden_size, dec_dropout).to(device)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
        "#optimizer = optim.Adam(params, lr=0.001)\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "QyssT-zakMo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8e218e-0858-4040-f6b1-65091675e28c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(data):\n",
        "    # Sort a data list by caption length (descending order).\n",
        "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    images, prompts = zip(*data)\n",
        "\n",
        "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Merge prompts (from tuple of 1D tensor to 2D tensor).\n",
        "    lengths = [len(prompt) for prompt in prompts]\n",
        "    padded_prompts = torch.zeros(len(prompts), max(lengths)).long()\n",
        "    for i, cap in enumerate(prompts):\n",
        "        end = lengths[i]\n",
        "        padded_prompts[i, :end] = cap[:end]\n",
        "\n",
        "    return images, padded_prompts, lengths"
      ],
      "metadata": {
        "id": "GJSfrDBNkTEQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "PwTH2GM2kXlg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = [ x[1] for x in next(iter(data_loader)) ]\n",
        "x[1]"
      ],
      "metadata": {
        "id": "uJDL1E2zzIqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c46173c-92f2-482f-c640-c2837af24d0c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  1,   6,  14,  64,   0, 386, 169, 116,   5,   6, 537, 389, 387, 388,\n",
              "        162,  17,   6, 538, 146, 191,   0, 158,  13,   2,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "id": "gyOC25N9msRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2bfb713-387d-4486-e34c-7b2962267508"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): EncoderCNN(\n",
              "    (resnet): ResNet(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      (fc): Linear(in_features=2048, out_features=256, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): DecoderRNN(\n",
              "    (embedding): Embedding(1484, 512)\n",
              "    (rnn): LSTM(512, 256, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=256, out_features=1484, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "q5LYzdyPmw85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5817e30c-1961-45f3-e417-3c1836bd9456"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 26,488,588 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "#TODO\n",
        "#criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "metadata": {
        "id": "_ffCzvbpmxOf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_output(output, word2index):\n",
        "    index2word = {index: word for word, index in word2index.items()}  # Create index-to-word dictionary\n",
        "    translated_sentences = []\n",
        "    for seq in output:\n",
        "        sentence = []\n",
        "        for idx in seq:\n",
        "            word = index2word.get(idx.item(), \"<UNK>\")\n",
        "            if word == \"<EOS>\":\n",
        "                break\n",
        "            sentence.append(word)\n",
        "        translated_sentence = \" \".join(sentence)\n",
        "        translated_sentences.append(translated_sentence)\n",
        "    return translated_sentences"
      ],
      "metadata": {
        "id": "V1YQPkVpGfGN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (images, prompts, trg_lengths) in enumerate(data_loader):\n",
        "    example_images = images\n",
        "    example_prompts = prompts\n",
        "    break\n",
        "\n",
        "# Translate the example prompt\n",
        "translated_example_prompts = translate_output(prompts, dataset.word2index)\n"
      ],
      "metadata": {
        "id": "y6XNJDCnLib5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_translations(images, prompts): \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Move images and prompts to the device\n",
        "        images = images.to(device)\n",
        "        prompts = prompts.to(device)\n",
        "\n",
        "        # Perform forward pass for the images and prompts\n",
        "        outputs = model(images, prompts)\n",
        "\n",
        "        # Get the predicted words with the highest probability\n",
        "        top1 = outputs.argmax(2).transpose(0, 1)\n",
        "\n",
        "        # Translate the predicted output to words\n",
        "        translated_output = translate_output(top1, dataset.word2index)\n",
        "\n",
        "        return translated_output\n",
        "    "
      ],
      "metadata": {
        "id": "DQefUCazLPZk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "translations_list = []  # List to store translated sentences\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (images, prompts, trg_lengths) in enumerate(data_loader):\n",
        "        images = images.to(device)\n",
        "        prompts = prompts.to(device)\n",
        "\n",
        "        # TODO add packing?\n",
        "        targets = pack_padded_sequence(prompts, trg_lengths, batch_first=True)[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images, prompts)\n",
        "\n",
        "        # Remove the <sos> token and reshape the output and target tensors\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim).contiguous()\n",
        "\n",
        "        trg = prompts.transpose(0, 1)[1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
        "                  .format(epoch + 1, num_epochs, i, len(data_loader), loss.item(), np.exp(loss.item())))\n",
        "\n",
        "    # Get translations after each epoch and append to the list\n",
        "    translations_list.append(get_translations(example_images, example_prompts))"
      ],
      "metadata": {
        "id": "2WryE8q3oZLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f799da-e074-4f43-8b05-9c426abb02ee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [0/500], Loss: 7.3156, Perplexity: 1503.5406\n",
            "Epoch [1/10], Step [50/500], Loss: 5.2023, Perplexity: 181.6851\n",
            "Epoch [1/10], Step [100/500], Loss: 4.2118, Perplexity: 67.4762\n",
            "Epoch [1/10], Step [150/500], Loss: 3.3689, Perplexity: 29.0469\n",
            "Epoch [1/10], Step [200/500], Loss: 4.7000, Perplexity: 109.9488\n",
            "Epoch [1/10], Step [250/500], Loss: 3.6381, Perplexity: 38.0210\n",
            "Epoch [1/10], Step [300/500], Loss: 3.5041, Perplexity: 33.2519\n",
            "Epoch [1/10], Step [350/500], Loss: 3.6949, Perplexity: 40.2407\n",
            "Epoch [1/10], Step [400/500], Loss: 4.1044, Perplexity: 60.6058\n",
            "Epoch [1/10], Step [450/500], Loss: 3.1979, Perplexity: 24.4803\n",
            "Epoch [2/10], Step [0/500], Loss: 2.9493, Perplexity: 19.0924\n",
            "Epoch [2/10], Step [50/500], Loss: 2.3402, Perplexity: 10.3836\n",
            "Epoch [2/10], Step [100/500], Loss: 4.4046, Perplexity: 81.8290\n",
            "Epoch [2/10], Step [150/500], Loss: 3.7858, Perplexity: 44.0709\n",
            "Epoch [2/10], Step [200/500], Loss: 3.6382, Perplexity: 38.0223\n",
            "Epoch [2/10], Step [250/500], Loss: 3.7721, Perplexity: 43.4699\n",
            "Epoch [2/10], Step [300/500], Loss: 3.0182, Perplexity: 20.4536\n",
            "Epoch [2/10], Step [350/500], Loss: 4.0239, Perplexity: 55.9168\n",
            "Epoch [2/10], Step [400/500], Loss: 2.9598, Perplexity: 19.2950\n",
            "Epoch [2/10], Step [450/500], Loss: 3.4398, Perplexity: 31.1804\n",
            "Epoch [3/10], Step [0/500], Loss: 4.5657, Perplexity: 96.1264\n",
            "Epoch [3/10], Step [50/500], Loss: 2.3831, Perplexity: 10.8388\n",
            "Epoch [3/10], Step [100/500], Loss: 3.5777, Perplexity: 35.7905\n",
            "Epoch [3/10], Step [150/500], Loss: 2.0309, Perplexity: 7.6208\n",
            "Epoch [3/10], Step [200/500], Loss: 3.3825, Perplexity: 29.4446\n",
            "Epoch [3/10], Step [250/500], Loss: 3.8893, Perplexity: 48.8767\n",
            "Epoch [3/10], Step [300/500], Loss: 3.8874, Perplexity: 48.7822\n",
            "Epoch [3/10], Step [350/500], Loss: 2.8664, Perplexity: 17.5743\n",
            "Epoch [3/10], Step [400/500], Loss: 2.9454, Perplexity: 19.0184\n",
            "Epoch [3/10], Step [450/500], Loss: 4.1237, Perplexity: 61.7862\n",
            "Epoch [4/10], Step [0/500], Loss: 2.3211, Perplexity: 10.1865\n",
            "Epoch [4/10], Step [50/500], Loss: 2.7354, Perplexity: 15.4164\n",
            "Epoch [4/10], Step [100/500], Loss: 1.6310, Perplexity: 5.1092\n",
            "Epoch [4/10], Step [150/500], Loss: 2.6951, Perplexity: 14.8071\n",
            "Epoch [4/10], Step [200/500], Loss: 2.1204, Perplexity: 8.3342\n",
            "Epoch [4/10], Step [250/500], Loss: 4.1283, Perplexity: 62.0714\n",
            "Epoch [4/10], Step [300/500], Loss: 2.2498, Perplexity: 9.4854\n",
            "Epoch [4/10], Step [350/500], Loss: 1.9375, Perplexity: 6.9411\n",
            "Epoch [4/10], Step [400/500], Loss: 3.0735, Perplexity: 21.6176\n",
            "Epoch [4/10], Step [450/500], Loss: 4.3031, Perplexity: 73.9260\n",
            "Epoch [5/10], Step [0/500], Loss: 2.0454, Perplexity: 7.7325\n",
            "Epoch [5/10], Step [50/500], Loss: 2.8327, Perplexity: 16.9918\n",
            "Epoch [5/10], Step [100/500], Loss: 3.0320, Perplexity: 20.7397\n",
            "Epoch [5/10], Step [150/500], Loss: 3.0996, Perplexity: 22.1901\n",
            "Epoch [5/10], Step [200/500], Loss: 2.9890, Perplexity: 19.8655\n",
            "Epoch [5/10], Step [250/500], Loss: 3.3005, Perplexity: 27.1263\n",
            "Epoch [5/10], Step [300/500], Loss: 2.4101, Perplexity: 11.1346\n",
            "Epoch [5/10], Step [350/500], Loss: 1.4562, Perplexity: 4.2895\n",
            "Epoch [5/10], Step [400/500], Loss: 1.6182, Perplexity: 5.0441\n",
            "Epoch [5/10], Step [450/500], Loss: 3.7649, Perplexity: 43.1615\n",
            "Epoch [6/10], Step [0/500], Loss: 1.8740, Perplexity: 6.5141\n",
            "Epoch [6/10], Step [50/500], Loss: 2.7473, Perplexity: 15.5999\n",
            "Epoch [6/10], Step [100/500], Loss: 2.6221, Perplexity: 13.7643\n",
            "Epoch [6/10], Step [150/500], Loss: 3.6418, Perplexity: 38.1587\n",
            "Epoch [6/10], Step [200/500], Loss: 2.1539, Perplexity: 8.6187\n",
            "Epoch [6/10], Step [250/500], Loss: 1.2386, Perplexity: 3.4508\n",
            "Epoch [6/10], Step [300/500], Loss: 0.7347, Perplexity: 2.0849\n",
            "Epoch [6/10], Step [350/500], Loss: 1.6565, Perplexity: 5.2410\n",
            "Epoch [6/10], Step [400/500], Loss: 2.8269, Perplexity: 16.8929\n",
            "Epoch [6/10], Step [450/500], Loss: 2.2046, Perplexity: 9.0668\n",
            "Epoch [7/10], Step [0/500], Loss: 3.0528, Perplexity: 21.1740\n",
            "Epoch [7/10], Step [50/500], Loss: 0.9519, Perplexity: 2.5906\n",
            "Epoch [7/10], Step [100/500], Loss: 2.0303, Perplexity: 7.6165\n",
            "Epoch [7/10], Step [150/500], Loss: 2.9283, Perplexity: 18.6964\n",
            "Epoch [7/10], Step [200/500], Loss: 1.5772, Perplexity: 4.8415\n",
            "Epoch [7/10], Step [250/500], Loss: 2.0470, Perplexity: 7.7445\n",
            "Epoch [7/10], Step [300/500], Loss: 1.7146, Perplexity: 5.5546\n",
            "Epoch [7/10], Step [350/500], Loss: 1.5696, Perplexity: 4.8046\n",
            "Epoch [7/10], Step [400/500], Loss: 2.5788, Perplexity: 13.1817\n",
            "Epoch [7/10], Step [450/500], Loss: 2.2320, Perplexity: 9.3186\n",
            "Epoch [8/10], Step [0/500], Loss: 3.4570, Perplexity: 31.7211\n",
            "Epoch [8/10], Step [50/500], Loss: 3.5354, Perplexity: 34.3093\n",
            "Epoch [8/10], Step [100/500], Loss: 2.5064, Perplexity: 12.2601\n",
            "Epoch [8/10], Step [150/500], Loss: 2.9264, Perplexity: 18.6605\n",
            "Epoch [8/10], Step [200/500], Loss: 2.2028, Perplexity: 9.0501\n",
            "Epoch [8/10], Step [250/500], Loss: 3.8047, Perplexity: 44.9095\n",
            "Epoch [8/10], Step [300/500], Loss: 1.4131, Perplexity: 4.1086\n",
            "Epoch [8/10], Step [350/500], Loss: 1.1141, Perplexity: 3.0468\n",
            "Epoch [8/10], Step [400/500], Loss: 3.2019, Perplexity: 24.5800\n",
            "Epoch [8/10], Step [450/500], Loss: 1.6874, Perplexity: 5.4054\n",
            "Epoch [9/10], Step [0/500], Loss: 1.6690, Perplexity: 5.3071\n",
            "Epoch [9/10], Step [50/500], Loss: 1.3126, Perplexity: 3.7160\n",
            "Epoch [9/10], Step [100/500], Loss: 3.6990, Perplexity: 40.4064\n",
            "Epoch [9/10], Step [150/500], Loss: 0.6382, Perplexity: 1.8930\n",
            "Epoch [9/10], Step [200/500], Loss: 1.8280, Perplexity: 6.2211\n",
            "Epoch [9/10], Step [250/500], Loss: 1.6814, Perplexity: 5.3730\n",
            "Epoch [9/10], Step [300/500], Loss: 0.6188, Perplexity: 1.8566\n",
            "Epoch [9/10], Step [350/500], Loss: 0.7902, Perplexity: 2.2038\n",
            "Epoch [9/10], Step [400/500], Loss: 1.6101, Perplexity: 5.0035\n",
            "Epoch [9/10], Step [450/500], Loss: 0.7658, Perplexity: 2.1508\n",
            "Epoch [10/10], Step [0/500], Loss: 1.2056, Perplexity: 3.3388\n",
            "Epoch [10/10], Step [50/500], Loss: 3.1929, Perplexity: 24.3583\n",
            "Epoch [10/10], Step [100/500], Loss: 0.5151, Perplexity: 1.6738\n",
            "Epoch [10/10], Step [150/500], Loss: 0.9333, Perplexity: 2.5429\n",
            "Epoch [10/10], Step [200/500], Loss: 1.6978, Perplexity: 5.4618\n",
            "Epoch [10/10], Step [250/500], Loss: 2.2975, Perplexity: 9.9492\n",
            "Epoch [10/10], Step [300/500], Loss: 2.2123, Perplexity: 9.1365\n",
            "Epoch [10/10], Step [350/500], Loss: 0.5017, Perplexity: 1.6516\n",
            "Epoch [10/10], Step [400/500], Loss: 0.5151, Perplexity: 1.6739\n",
            "Epoch [10/10], Step [450/500], Loss: 1.5227, Perplexity: 4.5846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translations_first_example, translations_second_example = zip(*translations_list)\n",
        "\n",
        "print(\"Translated first Prompt:\")\n",
        "print(translated_example_prompts[0])\n",
        "\n",
        "\n",
        "print(\"Translated outputs over epochs:\")\n",
        "print()\n",
        "# Print the translated prompts over epochs\n",
        "for i, sentence in enumerate(translations_first_example):\n",
        "    print(f\"Epoch {i+1} predicted sentence {sentence}\")\n",
        "\n",
        "#print()\n",
        "\n",
        "#print(\"Translated second Prompt:\")\n",
        "#print(translated_example_prompts[0])\n",
        "\n",
        "# Print the translated prompts over epochs\n",
        "#for i, sentence in enumerate(translations_first_example):\n",
        "#    print(f\"Epoch {i+1} predicted sentence {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YukQyqEGKqyi",
        "outputId": "38b4c0ba-4781-480e-b299-3ded85221ecb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated first Prompt:\n",
            "<SOS> full body of beautiful baroque girl <UNK> soft lighting <UNK> realistic wide angle <UNK> glamour pose <UNK> sharp focus <UNK> <UNK> k high <UNK> <UNK> <UNK> megapixel <UNK> insanely detailed <UNK> intricate <UNK> elegant <UNK> art by artgerm and wlop\n",
            "Translated outputs over epochs:\n",
            "\n",
            "Epoch 1 predicted sentence <UNK> a of of <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "Epoch 2 predicted sentence <UNK> a epic of a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> artgerm and <UNK> <UNK>\n",
            "Epoch 3 predicted sentence <UNK> a of of <UNK> <UNK> <UNK> <UNK> <UNK> lighting <UNK> <UNK> <UNK> <UNK> <UNK> sharp focus <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> by <UNK> artstation <UNK>\n",
            "Epoch 4 predicted sentence <UNK> a beautiful of a <UNK> <UNK> <UNK> <UNK> lighting <UNK> <UNK> <UNK> <UNK> <UNK> sharp light <UNK> <UNK> k <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> detailed <UNK> sharp <UNK> <UNK> <UNK> <UNK> <UNK> adrian ghenie and <UNK>\n",
            "Epoch 5 predicted sentence <UNK> a of a a futuristic street scene <UNK> lighting <UNK> <UNK> <UNK> angle <UNK> sharp focus <UNK> <UNK> k <UNK> <UNK> k <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> detailed <UNK> intricate <UNK> <UNK> masterpiece <UNK> by artgerm and wlop\n",
            "Epoch 6 predicted sentence <UNK> a beautiful sketch <UNK> <UNK> high <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> angle <UNK> sharp focus <UNK> sharp focus <UNK> <UNK> k <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> insanely detailed <UNK> intricate <UNK> <UNK> <UNK> art by artgerm and wlop\n",
            "Epoch 7 predicted sentence <UNK> a of of a baroque girl <UNK> soft lighting <UNK> realistic wide angle <UNK> sharp focus <UNK> sharp focus <UNK> <UNK> k high <UNK> <UNK> <UNK> megapixel <UNK> insanely detailed <UNK> intricate <UNK> elegant <UNK> art by artgerm and wlop\n",
            "Epoch 8 predicted sentence <UNK> a body of beautiful baroque girl <UNK> soft lighting <UNK> realistic wide angle <UNK> sharp pose <UNK> sharp focus <UNK> <UNK> k high <UNK> <UNK> <UNK> megapixel <UNK> insanely detailed <UNK> intricate <UNK> elegant <UNK> art by artgerm and wlop\n",
            "Epoch 9 predicted sentence <UNK> a body of beautiful baroque girl <UNK> soft lighting <UNK> realistic wide angle <UNK> glamour pose <UNK> sharp focus <UNK> <UNK> k high <UNK> <UNK> <UNK> megapixel <UNK> insanely detailed <UNK> intricate <UNK> elegant <UNK> art by artgerm and wlop\n",
            "Epoch 10 predicted sentence <UNK> a body of beautiful baroque girl <UNK> soft lighting <UNK> realistic wide angle <UNK> sharp focus <UNK> <UNK> k high <UNK> <UNK> high <UNK> <UNK> <UNK> megapixel <UNK> insanely detailed <UNK> intricate <UNK> elegant <UNK> art by artgerm and wlop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = load_dataset('poloclub/diffusiondb', 'large_random_1k')\n",
        "test_df = pd.DataFrame(testset[\"train\"])\n",
        "test_df = train_df[[\"image\", \"prompt\"]]\n",
        "del testset\n",
        "test_df"
      ],
      "metadata": {
        "id": "RVH7yKSx00mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DiffusionDataset(test_df, batch_size = batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "ptsT7f2P07T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  losses = []\n",
        "  for (images, prompts) in test_loader:\n",
        "    images = images.to(device)\n",
        "    prompts = prompts.to(device)\n",
        "    output = model(images, prompts)\n",
        "\n",
        "    output_dim = output.shape[-1]\n",
        "    output = output[1:].view(-1, output_dim).contiguous() \n",
        "\n",
        "    trg = prompts.transpose(0, 1)  # Transpose dimensions 0 and 1\n",
        "    trg = trg[1:].contiguous()  # Remove the first token <sos>\n",
        "    trg = trg.view(-1)  # Flatten the tensor\n",
        "\n",
        "    loss = criterion(output, targets)\n",
        "    losses.append(loss.item())\n",
        "    \n",
        "avg_loss = np.sum(losses) / len(test_loader)\n",
        "print(f\"Avg Loss: {avg_loss}, Avg Perplexity: {np.exp(avg_loss)}\")"
      ],
      "metadata": {
        "id": "4lf_VRHJ09Xg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}