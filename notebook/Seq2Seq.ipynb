{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i323PgRmbOo4",
        "outputId": "80f04fe1-6b58-4644-fecd-a9fc657f37b9"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z3HcVEM4bTzn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fruda\\anaconda3\\envs\\AI-Lab\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np \n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import spacy\n",
        "from random import seed\n",
        "from random import random\n",
        "import torchtext\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt-M8MDXumpE",
        "outputId": "86022df4-52ff-4219-f11d-a4bc3832f397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488,
          "referenced_widgets": [
            "a0941e17d962401f9514f3d87df0a65f",
            "e311b0f9685146958444dcdc7870d099",
            "8d77e6f4b38c41f0af41b07b0be3c107",
            "fd3d00547fe54bcda39f22a6b18b7fc4",
            "4475e962b705495195340471881ea91c",
            "f31cea9b62204578a45548a972364780",
            "039f546913984c7ca4de05d891370828",
            "f75e2f36640c4154b7b2363a57143b9d",
            "9eba77a7857a4310b4bfe37403d3736e",
            "79b58f471c4342feb83e107166be8651",
            "5d42d7d8daac427db6a054141236e03d"
          ]
        },
        "id": "sjwrT_ZqbXnZ",
        "outputId": "ec35e7b4-3edb-46f2-84b8-1b1f3ec05273"
      },
      "outputs": [],
      "source": [
        "data_path = \"preprocessed\"\n",
        "word2index_path = \"word2index.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "umn7FrIbbqeo"
      },
      "outputs": [],
      "source": [
        "class DiffusionDataset(Dataset):\n",
        "    def __init__(self, data_path, word2index_path, train_split_ratio=0.8, train=True):\n",
        "        self.data_path = data_path\n",
        "        with open(word2index_path, 'rb') as f:\n",
        "            self.word2index = pickle.load(f)\n",
        "        self.vocab_size = len(self.word2index)\n",
        "        self.eos_index = self.word2index[\"<EOS>\"]\n",
        "        self.train = train\n",
        "        self.train_split_ratio = train_split_ratio\n",
        "        self.files = os.listdir(data_path)\n",
        "\n",
        "        n_image_prompt = int(len(self.files) / 2) # len(self.files) must be an even number\n",
        "\n",
        "        # Calculate the split index\n",
        "        self.split_index = int((self.train_split_ratio * n_image_prompt))\n",
        "        \n",
        "        # Calculate the total number of rows\n",
        "        self.total_rows = 0\n",
        "\n",
        "        if not train:\n",
        "            self.total_rows = n_image_prompt - self.split_index\n",
        "        else:\n",
        "            self.total_rows = self.split_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_rows\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO: Recalculate the preprocessed data and then remove the +1 everywhere in this function\n",
        "        if isinstance(idx, int):\n",
        "            image = np.load(f\"{data_path}\\\\image_{idx + 1}.npy\")\n",
        "            prompt = np.load(f\"{data_path}\\\\prompt_{idx + 1}.npy\")\n",
        "            return torch.tensor(image), torch.tensor(prompt)\n",
        "        else:\n",
        "            image_batch = []\n",
        "            prompt_batch = []\n",
        "            for i in idx:\n",
        "                image = np.load(f\"{data_path}\\\\image_{i + 1}.npy\")\n",
        "                prompt = np.load(f\"{data_path}\\\\prompt_{i + 1}.npy\")\n",
        "                image_batch.append(image)\n",
        "                prompt_batch.append(prompt)\n",
        "                return torch.tensor(image_batch), torch.tensor(prompt_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mtuvvCgbbtQy"
      },
      "outputs": [],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, n_layers, hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, hid_dim)\n",
        "\n",
        "        self.hidden_size = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "\n",
        "        batch_size = features.size(0)\n",
        "        hidden = features.unsqueeze(0).expand(self.n_layers, batch_size, self.hidden_size)\n",
        "        # Initialize the cell state with zeros\n",
        "        cell = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(features.device)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, n_layers, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden.contiguous(), cell.contiguous()))\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C2GYGB9QkiTl"
      },
      "outputs": [],
      "source": [
        "from torch.jit import script_if_tracing\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "       \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = float(0.5)):\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token)\n",
        "\n",
        "            input = trg[:,t] if random() < teacher_forcing_ratio else top1\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = DiffusionDataset(data_path, word2index_path, train_split_ratio=0.8, train=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w-0WbtFHlEla"
      },
      "outputs": [],
      "source": [
        "embed_size = 512\n",
        "hidden_size = 256\n",
        "output_size = dataset.vocab_size\n",
        "n_layers = 2\n",
        "dec_dropout = 0.5\n",
        "\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "clip = 1\n",
        "\n",
        "# seed random number generator\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkzZs6BmkH36",
        "outputId": "dae949b1-226e-43b1-c765-57805c6b575e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10003"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyssT-zakMo_",
        "outputId": "6a8e218e-0858-4040-f6b1-65091675e28c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fruda\\anaconda3\\envs\\AI-Lab\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\fruda\\anaconda3\\envs\\AI-Lab\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "encoder = EncoderCNN(embed_size, n_layers, hidden_size).to(device)\n",
        "decoder = DecoderRNN(output_size, embed_size, n_layers, hidden_size, dec_dropout).to(device)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GJSfrDBNkTEQ"
      },
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "    # Sort a data list by caption length (descending order).\n",
        "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    images, prompts = zip(*data)\n",
        "\n",
        "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Merge prompts (from tuple of 1D tensor to 2D tensor).\n",
        "    lengths = [len(prompt) for prompt in prompts]\n",
        "    padded_prompts = torch.zeros(len(prompts), max(lengths)).long()\n",
        "    for i, cap in enumerate(prompts):\n",
        "        end = lengths[i]\n",
        "        padded_prompts[i, :end] = cap[:end]\n",
        "\n",
        "    return images, padded_prompts, lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PwTH2GM2kXlg"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJDL1E2zzIqe",
        "outputId": "9c46173c-92f2-482f-c640-c2837af24d0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([   1,  145, 2074, 1225, 1289,  643,    0, 1684,    0, 1143,    0,  628,\n",
              "        2657,    0,    0,   14,   30,   24,    0, 1502,    0,  125,    6, 4016,\n",
              "           0,   91,   34,    0,    0,   14,   67,   66,    0,   38,    0,   85,\n",
              "           0, 2455,  746,    0, 4340,    6,   89, 4341,    0, 1077, 1522,    0,\n",
              "        3745,  150, 4139,    0, 2282, 4140,    0, 1902,  480, 1950,    0, 3064,\n",
              "        1737,    0, 3388,    0, 3505,    0, 2701, 4988,    2,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = [ x[1] for x in next(iter(data_loader)) ]\n",
        "x[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyOC25N9msRE",
        "outputId": "e2bfb713-387d-4486-e34c-7b2962267508"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): EncoderCNN(\n",
              "    (resnet): ResNet(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      (fc): Linear(in_features=2048, out_features=256, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): DecoderRNN(\n",
              "    (embedding): Embedding(10003, 512)\n",
              "    (rnn): LSTM(512, 256, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=256, out_features=10003, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5LYzdyPmw85",
        "outputId": "5817e30c-1961-45f3-e417-3c1836bd9456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 33,039,699 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_ffCzvbpmxOf"
      },
      "outputs": [],
      "source": [
        "#TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "#TODO\n",
        "#criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "V1YQPkVpGfGN"
      },
      "outputs": [],
      "source": [
        "def translate_output(output, word2index):\n",
        "    index2word = {index: word for word, index in word2index.items()}  # Create index-to-word dictionary\n",
        "    translated_sentences = []\n",
        "    for seq in output:\n",
        "        sentence = []\n",
        "        for idx in seq:\n",
        "            word = index2word.get(idx.item(), \"<UNK>\")\n",
        "            if word == \"<EOS>\":\n",
        "                break\n",
        "            sentence.append(word)\n",
        "        translated_sentence = \" \".join(sentence)\n",
        "        translated_sentences.append(translated_sentence)\n",
        "    return translated_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "y6XNJDCnLib5"
      },
      "outputs": [],
      "source": [
        "for i, (images, prompts, trg_lengths) in enumerate(data_loader):\n",
        "    example_images = images\n",
        "    example_prompts = prompts\n",
        "    break\n",
        "\n",
        "# Translate the example prompt\n",
        "translated_example_prompts = translate_output(prompts, dataset.word2index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DQefUCazLPZk"
      },
      "outputs": [],
      "source": [
        "def get_translations(images, prompts): \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Move images and prompts to the device\n",
        "        images = images.to(device)\n",
        "        prompts = prompts.to(device)\n",
        "\n",
        "        # Perform forward pass for the images and prompts\n",
        "        outputs = model(images, prompts)\n",
        "\n",
        "        # Get the predicted words with the highest probability\n",
        "        top1 = outputs.argmax(2).transpose(0, 1)\n",
        "\n",
        "        # Translate the predicted output to words\n",
        "        translated_output = translate_output(top1, dataset.word2index)\n",
        "\n",
        "        return translated_output\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WryE8q3oZLC",
        "outputId": "65f799da-e074-4f43-8b05-9c426abb02ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [0/2500], Loss: 9.2096, Perplexity: 9992.6797\n",
            "Epoch [1/10], Step [32/2500], Loss: 2.8922, Perplexity: 18.0333\n",
            "Epoch [1/10], Step [64/2500], Loss: 2.7286, Perplexity: 15.3118\n",
            "Epoch [1/10], Step [96/2500], Loss: 2.3791, Perplexity: 10.7954\n",
            "Epoch [1/10], Step [128/2500], Loss: 2.0263, Perplexity: 7.5857\n",
            "Epoch [1/10], Step [160/2500], Loss: 2.9899, Perplexity: 19.8828\n",
            "Epoch [1/10], Step [192/2500], Loss: 2.5203, Perplexity: 12.4324\n",
            "Epoch [1/10], Step [224/2500], Loss: 2.3879, Perplexity: 10.8908\n",
            "Epoch [1/10], Step [256/2500], Loss: 2.7214, Perplexity: 15.2020\n",
            "Epoch [1/10], Step [288/2500], Loss: 1.9260, Perplexity: 6.8623\n",
            "Epoch [1/10], Step [320/2500], Loss: 2.7853, Perplexity: 16.2046\n",
            "Epoch [1/10], Step [352/2500], Loss: 2.7107, Perplexity: 15.0397\n",
            "Epoch [1/10], Step [384/2500], Loss: 2.8930, Perplexity: 18.0482\n",
            "Epoch [1/10], Step [416/2500], Loss: 2.2851, Perplexity: 9.8268\n",
            "Epoch [1/10], Step [448/2500], Loss: 2.3994, Perplexity: 11.0167\n",
            "Epoch [1/10], Step [480/2500], Loss: 2.7288, Perplexity: 15.3151\n",
            "Epoch [1/10], Step [512/2500], Loss: 1.9416, Perplexity: 6.9700\n",
            "Epoch [1/10], Step [544/2500], Loss: 2.9975, Perplexity: 20.0347\n",
            "Epoch [1/10], Step [576/2500], Loss: 2.4082, Perplexity: 11.1144\n",
            "Epoch [1/10], Step [608/2500], Loss: 2.8317, Perplexity: 16.9747\n",
            "Epoch [1/10], Step [640/2500], Loss: 2.0127, Perplexity: 7.4839\n",
            "Epoch [1/10], Step [672/2500], Loss: 2.3632, Perplexity: 10.6249\n",
            "Epoch [1/10], Step [704/2500], Loss: 2.1463, Perplexity: 8.5530\n",
            "Epoch [1/10], Step [736/2500], Loss: 2.4553, Perplexity: 11.6504\n",
            "Epoch [1/10], Step [768/2500], Loss: 1.4726, Perplexity: 4.3607\n",
            "Epoch [1/10], Step [800/2500], Loss: 2.7147, Perplexity: 15.1001\n",
            "Epoch [1/10], Step [832/2500], Loss: 2.0624, Perplexity: 7.8650\n",
            "Epoch [1/10], Step [864/2500], Loss: 1.7593, Perplexity: 5.8082\n",
            "Epoch [1/10], Step [896/2500], Loss: 2.1305, Perplexity: 8.4188\n",
            "Epoch [1/10], Step [928/2500], Loss: 0.7072, Perplexity: 2.0284\n",
            "Epoch [1/10], Step [960/2500], Loss: 2.2682, Perplexity: 9.6615\n",
            "Epoch [1/10], Step [992/2500], Loss: 2.3661, Perplexity: 10.6553\n",
            "Epoch [1/10], Step [1024/2500], Loss: 1.8731, Perplexity: 6.5082\n",
            "Epoch [1/10], Step [1056/2500], Loss: 2.3745, Perplexity: 10.7454\n",
            "Epoch [1/10], Step [1088/2500], Loss: 2.2944, Perplexity: 9.9181\n",
            "Epoch [1/10], Step [1120/2500], Loss: 1.5637, Perplexity: 4.7764\n",
            "Epoch [1/10], Step [1152/2500], Loss: 2.1231, Perplexity: 8.3566\n",
            "Epoch [1/10], Step [1184/2500], Loss: 2.1470, Perplexity: 8.5593\n",
            "Epoch [1/10], Step [1216/2500], Loss: 2.3428, Perplexity: 10.4104\n",
            "Epoch [1/10], Step [1248/2500], Loss: 2.1676, Perplexity: 8.7376\n",
            "Epoch [1/10], Step [1280/2500], Loss: 2.4615, Perplexity: 11.7223\n",
            "Epoch [1/10], Step [1312/2500], Loss: 2.8412, Perplexity: 17.1368\n",
            "Epoch [1/10], Step [1344/2500], Loss: 2.1311, Perplexity: 8.4244\n",
            "Epoch [1/10], Step [1376/2500], Loss: 1.8290, Perplexity: 6.2278\n",
            "Epoch [1/10], Step [1408/2500], Loss: 2.3860, Perplexity: 10.8698\n",
            "Epoch [1/10], Step [1440/2500], Loss: 1.9228, Perplexity: 6.8401\n",
            "Epoch [1/10], Step [1472/2500], Loss: 2.4288, Perplexity: 11.3449\n",
            "Epoch [1/10], Step [1504/2500], Loss: 2.1800, Perplexity: 8.8464\n",
            "Epoch [1/10], Step [1536/2500], Loss: 2.3796, Perplexity: 10.8010\n",
            "Epoch [1/10], Step [1568/2500], Loss: 1.5715, Perplexity: 4.8138\n",
            "Epoch [1/10], Step [1600/2500], Loss: 1.9155, Perplexity: 6.7904\n",
            "Epoch [1/10], Step [1632/2500], Loss: 2.2883, Perplexity: 9.8585\n",
            "Epoch [1/10], Step [1664/2500], Loss: 2.1450, Perplexity: 8.5419\n",
            "Epoch [1/10], Step [1696/2500], Loss: 1.5144, Perplexity: 4.5467\n",
            "Epoch [1/10], Step [1728/2500], Loss: 2.0352, Perplexity: 7.6541\n",
            "Epoch [1/10], Step [1760/2500], Loss: 1.6515, Perplexity: 5.2148\n",
            "Epoch [1/10], Step [1792/2500], Loss: 1.8907, Perplexity: 6.6239\n",
            "Epoch [1/10], Step [1824/2500], Loss: 2.0552, Perplexity: 7.8081\n",
            "Epoch [1/10], Step [1856/2500], Loss: 1.6118, Perplexity: 5.0118\n",
            "Epoch [1/10], Step [1888/2500], Loss: 1.9322, Perplexity: 6.9044\n",
            "Epoch [1/10], Step [1920/2500], Loss: 1.9217, Perplexity: 6.8325\n",
            "Epoch [1/10], Step [1952/2500], Loss: 1.9727, Perplexity: 7.1903\n",
            "Epoch [1/10], Step [1984/2500], Loss: 1.4546, Perplexity: 4.2827\n",
            "Epoch [1/10], Step [2016/2500], Loss: 1.6756, Perplexity: 5.3422\n",
            "Epoch [1/10], Step [2048/2500], Loss: 1.4801, Perplexity: 4.3932\n",
            "Epoch [1/10], Step [2080/2500], Loss: 1.5631, Perplexity: 4.7738\n",
            "Epoch [1/10], Step [2112/2500], Loss: 1.5641, Perplexity: 4.7786\n",
            "Epoch [1/10], Step [2144/2500], Loss: 1.7638, Perplexity: 5.8347\n",
            "Epoch [1/10], Step [2176/2500], Loss: 1.2787, Perplexity: 3.5920\n",
            "Epoch [1/10], Step [2208/2500], Loss: 2.3675, Perplexity: 10.6706\n",
            "Epoch [1/10], Step [2240/2500], Loss: 1.6037, Perplexity: 4.9713\n",
            "Epoch [1/10], Step [2272/2500], Loss: 2.3299, Perplexity: 10.2769\n",
            "Epoch [1/10], Step [2304/2500], Loss: 1.9444, Perplexity: 6.9896\n",
            "Epoch [1/10], Step [2336/2500], Loss: 1.7886, Perplexity: 5.9809\n",
            "Epoch [1/10], Step [2368/2500], Loss: 2.3927, Perplexity: 10.9427\n",
            "Epoch [1/10], Step [2400/2500], Loss: 1.7806, Perplexity: 5.9334\n",
            "Epoch [1/10], Step [2432/2500], Loss: 2.0530, Perplexity: 7.7916\n",
            "Epoch [1/10], Step [2464/2500], Loss: 1.7553, Perplexity: 5.7849\n",
            "Epoch [1/10], Step [2496/2500], Loss: 1.6271, Perplexity: 5.0891\n",
            "Epoch [2/10], Step [0/2500], Loss: 0.9280, Perplexity: 2.5294\n",
            "Epoch [2/10], Step [32/2500], Loss: 1.6154, Perplexity: 5.0301\n",
            "Epoch [2/10], Step [64/2500], Loss: 1.7833, Perplexity: 5.9496\n",
            "Epoch [2/10], Step [96/2500], Loss: 1.4903, Perplexity: 4.4386\n",
            "Epoch [2/10], Step [128/2500], Loss: 2.0336, Perplexity: 7.6417\n",
            "Epoch [2/10], Step [160/2500], Loss: 1.4038, Perplexity: 4.0708\n",
            "Epoch [2/10], Step [192/2500], Loss: 2.0169, Perplexity: 7.5148\n",
            "Epoch [2/10], Step [224/2500], Loss: 1.5158, Perplexity: 4.5529\n",
            "Epoch [2/10], Step [256/2500], Loss: 1.8792, Perplexity: 6.5479\n",
            "Epoch [2/10], Step [288/2500], Loss: 1.6869, Perplexity: 5.4025\n",
            "Epoch [2/10], Step [320/2500], Loss: 1.8963, Perplexity: 6.6615\n",
            "Epoch [2/10], Step [352/2500], Loss: 1.6209, Perplexity: 5.0577\n",
            "Epoch [2/10], Step [384/2500], Loss: 1.8297, Perplexity: 6.2321\n",
            "Epoch [2/10], Step [416/2500], Loss: 1.6706, Perplexity: 5.3153\n",
            "Epoch [2/10], Step [448/2500], Loss: 2.0301, Perplexity: 7.6145\n",
            "Epoch [2/10], Step [480/2500], Loss: 1.9398, Perplexity: 6.9574\n",
            "Epoch [2/10], Step [512/2500], Loss: 1.9559, Perplexity: 7.0701\n",
            "Epoch [2/10], Step [544/2500], Loss: 1.5167, Perplexity: 4.5570\n",
            "Epoch [2/10], Step [576/2500], Loss: 1.3179, Perplexity: 3.7357\n",
            "Epoch [2/10], Step [608/2500], Loss: 1.6572, Perplexity: 5.2448\n",
            "Epoch [2/10], Step [640/2500], Loss: 1.6805, Perplexity: 5.3684\n",
            "Epoch [2/10], Step [672/2500], Loss: 1.0631, Perplexity: 2.8953\n",
            "Epoch [2/10], Step [704/2500], Loss: 2.0219, Perplexity: 7.5523\n",
            "Epoch [2/10], Step [736/2500], Loss: 1.8221, Perplexity: 6.1848\n",
            "Epoch [2/10], Step [768/2500], Loss: 2.0078, Perplexity: 7.4467\n",
            "Epoch [2/10], Step [800/2500], Loss: 1.2047, Perplexity: 3.3356\n",
            "Epoch [2/10], Step [832/2500], Loss: 1.6224, Perplexity: 5.0650\n",
            "Epoch [2/10], Step [864/2500], Loss: 1.6076, Perplexity: 4.9908\n",
            "Epoch [2/10], Step [896/2500], Loss: 1.1971, Perplexity: 3.3106\n",
            "Epoch [2/10], Step [928/2500], Loss: 1.7692, Perplexity: 5.8662\n",
            "Epoch [2/10], Step [960/2500], Loss: 1.3748, Perplexity: 3.9545\n",
            "Epoch [2/10], Step [992/2500], Loss: 1.2468, Perplexity: 3.4791\n",
            "Epoch [2/10], Step [1024/2500], Loss: 1.2983, Perplexity: 3.6632\n",
            "Epoch [2/10], Step [1056/2500], Loss: 1.9683, Perplexity: 7.1584\n",
            "Epoch [2/10], Step [1088/2500], Loss: 2.1517, Perplexity: 8.5998\n",
            "Epoch [2/10], Step [1120/2500], Loss: 1.0777, Perplexity: 2.9379\n",
            "Epoch [2/10], Step [1152/2500], Loss: 1.6107, Perplexity: 5.0065\n",
            "Epoch [2/10], Step [1184/2500], Loss: 2.1450, Perplexity: 8.5424\n",
            "Epoch [2/10], Step [1216/2500], Loss: 1.6158, Perplexity: 5.0319\n",
            "Epoch [2/10], Step [1248/2500], Loss: 1.5351, Perplexity: 4.6418\n",
            "Epoch [2/10], Step [1280/2500], Loss: 1.4989, Perplexity: 4.4766\n",
            "Epoch [2/10], Step [1312/2500], Loss: 1.6521, Perplexity: 5.2178\n",
            "Epoch [2/10], Step [1344/2500], Loss: 1.2330, Perplexity: 3.4316\n",
            "Epoch [2/10], Step [1376/2500], Loss: 1.6217, Perplexity: 5.0619\n",
            "Epoch [2/10], Step [1408/2500], Loss: 1.6898, Perplexity: 5.4185\n",
            "Epoch [2/10], Step [1440/2500], Loss: 1.7009, Perplexity: 5.4786\n",
            "Epoch [2/10], Step [1472/2500], Loss: 1.8149, Perplexity: 6.1404\n",
            "Epoch [2/10], Step [1504/2500], Loss: 1.8417, Perplexity: 6.3076\n",
            "Epoch [2/10], Step [1536/2500], Loss: 1.4289, Perplexity: 4.1743\n",
            "Epoch [2/10], Step [1568/2500], Loss: 1.8772, Perplexity: 6.5350\n",
            "Epoch [2/10], Step [1600/2500], Loss: 1.3634, Perplexity: 3.9096\n",
            "Epoch [2/10], Step [1632/2500], Loss: 1.6762, Perplexity: 5.3450\n",
            "Epoch [2/10], Step [1664/2500], Loss: 1.4186, Perplexity: 4.1314\n",
            "Epoch [2/10], Step [1696/2500], Loss: 1.7924, Perplexity: 6.0036\n",
            "Epoch [2/10], Step [1728/2500], Loss: 1.4421, Perplexity: 4.2295\n",
            "Epoch [2/10], Step [1760/2500], Loss: 1.4466, Perplexity: 4.2486\n",
            "Epoch [2/10], Step [1792/2500], Loss: 1.4078, Perplexity: 4.0870\n",
            "Epoch [2/10], Step [1824/2500], Loss: 1.6277, Perplexity: 5.0923\n",
            "Epoch [2/10], Step [1856/2500], Loss: 1.3906, Perplexity: 4.0174\n",
            "Epoch [2/10], Step [1888/2500], Loss: 1.5901, Perplexity: 4.9040\n",
            "Epoch [2/10], Step [1920/2500], Loss: 1.0524, Perplexity: 2.8645\n",
            "Epoch [2/10], Step [1952/2500], Loss: 1.5336, Perplexity: 4.6349\n",
            "Epoch [2/10], Step [1984/2500], Loss: 1.1484, Perplexity: 3.1531\n",
            "Epoch [2/10], Step [2016/2500], Loss: 1.8731, Perplexity: 6.5083\n",
            "Epoch [2/10], Step [2048/2500], Loss: 1.5219, Perplexity: 4.5808\n",
            "Epoch [2/10], Step [2080/2500], Loss: 1.1834, Perplexity: 3.2655\n",
            "Epoch [2/10], Step [2112/2500], Loss: 1.6059, Perplexity: 4.9823\n",
            "Epoch [2/10], Step [2144/2500], Loss: 1.2206, Perplexity: 3.3893\n",
            "Epoch [2/10], Step [2176/2500], Loss: 1.7836, Perplexity: 5.9513\n",
            "Epoch [2/10], Step [2208/2500], Loss: 1.9931, Perplexity: 7.3386\n",
            "Epoch [2/10], Step [2240/2500], Loss: 1.3248, Perplexity: 3.7614\n",
            "Epoch [2/10], Step [2272/2500], Loss: 1.4170, Perplexity: 4.1245\n",
            "Epoch [2/10], Step [2304/2500], Loss: 1.0021, Perplexity: 2.7240\n",
            "Epoch [2/10], Step [2336/2500], Loss: 1.4124, Perplexity: 4.1056\n",
            "Epoch [2/10], Step [2368/2500], Loss: 2.0054, Perplexity: 7.4288\n",
            "Epoch [2/10], Step [2400/2500], Loss: 1.6190, Perplexity: 5.0482\n",
            "Epoch [2/10], Step [2432/2500], Loss: 1.5092, Perplexity: 4.5229\n",
            "Epoch [2/10], Step [2464/2500], Loss: 2.0678, Perplexity: 7.9076\n",
            "Epoch [2/10], Step [2496/2500], Loss: 1.8974, Perplexity: 6.6686\n",
            "Epoch [3/10], Step [0/2500], Loss: 2.1141, Perplexity: 8.2825\n",
            "Epoch [3/10], Step [32/2500], Loss: 1.6912, Perplexity: 5.4257\n",
            "Epoch [3/10], Step [64/2500], Loss: 1.1959, Perplexity: 3.3066\n",
            "Epoch [3/10], Step [96/2500], Loss: 1.0779, Perplexity: 2.9385\n",
            "Epoch [3/10], Step [128/2500], Loss: 1.5114, Perplexity: 4.5332\n",
            "Epoch [3/10], Step [160/2500], Loss: 2.0865, Perplexity: 8.0569\n",
            "Epoch [3/10], Step [192/2500], Loss: 1.7471, Perplexity: 5.7382\n",
            "Epoch [3/10], Step [224/2500], Loss: 1.5151, Perplexity: 4.5497\n",
            "Epoch [3/10], Step [256/2500], Loss: 1.0774, Perplexity: 2.9372\n",
            "Epoch [3/10], Step [288/2500], Loss: 1.6328, Perplexity: 5.1183\n",
            "Epoch [3/10], Step [320/2500], Loss: 1.5090, Perplexity: 4.5221\n",
            "Epoch [3/10], Step [352/2500], Loss: 1.6569, Perplexity: 5.2431\n",
            "Epoch [3/10], Step [384/2500], Loss: 1.1800, Perplexity: 3.2544\n",
            "Epoch [3/10], Step [416/2500], Loss: 1.3608, Perplexity: 3.8994\n",
            "Epoch [3/10], Step [448/2500], Loss: 1.3891, Perplexity: 4.0112\n",
            "Epoch [3/10], Step [480/2500], Loss: 1.4890, Perplexity: 4.4325\n",
            "Epoch [3/10], Step [512/2500], Loss: 1.5757, Perplexity: 4.8343\n",
            "Epoch [3/10], Step [544/2500], Loss: 0.6425, Perplexity: 1.9013\n",
            "Epoch [3/10], Step [576/2500], Loss: 1.4204, Perplexity: 4.1386\n",
            "Epoch [3/10], Step [608/2500], Loss: 1.5729, Perplexity: 4.8207\n",
            "Epoch [3/10], Step [640/2500], Loss: 1.5953, Perplexity: 4.9297\n",
            "Epoch [3/10], Step [672/2500], Loss: 1.6303, Perplexity: 5.1055\n",
            "Epoch [3/10], Step [704/2500], Loss: 1.9226, Perplexity: 6.8390\n",
            "Epoch [3/10], Step [736/2500], Loss: 1.9660, Perplexity: 7.1421\n",
            "Epoch [3/10], Step [768/2500], Loss: 1.3555, Perplexity: 3.8786\n",
            "Epoch [3/10], Step [800/2500], Loss: 1.2987, Perplexity: 3.6646\n",
            "Epoch [3/10], Step [832/2500], Loss: 1.1446, Perplexity: 3.1411\n",
            "Epoch [3/10], Step [864/2500], Loss: 1.3174, Perplexity: 3.7336\n",
            "Epoch [3/10], Step [896/2500], Loss: 1.2640, Perplexity: 3.5395\n",
            "Epoch [3/10], Step [928/2500], Loss: 1.4930, Perplexity: 4.4504\n",
            "Epoch [3/10], Step [960/2500], Loss: 1.2215, Perplexity: 3.3922\n",
            "Epoch [3/10], Step [992/2500], Loss: 1.6967, Perplexity: 5.4559\n",
            "Epoch [3/10], Step [1024/2500], Loss: 1.4984, Perplexity: 4.4744\n",
            "Epoch [3/10], Step [1056/2500], Loss: 1.6924, Perplexity: 5.4325\n",
            "Epoch [3/10], Step [1088/2500], Loss: 0.8868, Perplexity: 2.4273\n",
            "Epoch [3/10], Step [1120/2500], Loss: 1.4522, Perplexity: 4.2726\n",
            "Epoch [3/10], Step [1152/2500], Loss: 1.4730, Perplexity: 4.3623\n",
            "Epoch [3/10], Step [1184/2500], Loss: 1.4597, Perplexity: 4.3047\n",
            "Epoch [3/10], Step [1216/2500], Loss: 1.5379, Perplexity: 4.6550\n",
            "Epoch [3/10], Step [1248/2500], Loss: 1.5852, Perplexity: 4.8801\n",
            "Epoch [3/10], Step [1280/2500], Loss: 1.4160, Perplexity: 4.1205\n",
            "Epoch [3/10], Step [1312/2500], Loss: 1.4950, Perplexity: 4.4594\n",
            "Epoch [3/10], Step [1344/2500], Loss: 1.2152, Perplexity: 3.3709\n",
            "Epoch [3/10], Step [1376/2500], Loss: 1.3113, Perplexity: 3.7108\n",
            "Epoch [3/10], Step [1408/2500], Loss: 1.7547, Perplexity: 5.7817\n",
            "Epoch [3/10], Step [1440/2500], Loss: 1.2652, Perplexity: 3.5439\n",
            "Epoch [3/10], Step [1472/2500], Loss: 0.9788, Perplexity: 2.6613\n",
            "Epoch [3/10], Step [1504/2500], Loss: 1.7770, Perplexity: 5.9120\n",
            "Epoch [3/10], Step [1536/2500], Loss: 1.4092, Perplexity: 4.0925\n",
            "Epoch [3/10], Step [1568/2500], Loss: 1.5856, Perplexity: 4.8824\n",
            "Epoch [3/10], Step [1600/2500], Loss: 1.3059, Perplexity: 3.6909\n",
            "Epoch [3/10], Step [1632/2500], Loss: 1.3449, Perplexity: 3.8377\n",
            "Epoch [3/10], Step [1664/2500], Loss: 1.2397, Perplexity: 3.4547\n",
            "Epoch [3/10], Step [1696/2500], Loss: 1.8986, Perplexity: 6.6765\n",
            "Epoch [3/10], Step [1728/2500], Loss: 1.5262, Perplexity: 4.6008\n",
            "Epoch [3/10], Step [1760/2500], Loss: 0.9532, Perplexity: 2.5940\n",
            "Epoch [3/10], Step [1792/2500], Loss: 1.3034, Perplexity: 3.6818\n",
            "Epoch [3/10], Step [1824/2500], Loss: 1.1238, Perplexity: 3.0765\n",
            "Epoch [3/10], Step [1856/2500], Loss: 1.5778, Perplexity: 4.8443\n",
            "Epoch [3/10], Step [1888/2500], Loss: 1.9292, Perplexity: 6.8843\n",
            "Epoch [3/10], Step [1920/2500], Loss: 1.2015, Perplexity: 3.3252\n",
            "Epoch [3/10], Step [1952/2500], Loss: 0.9339, Perplexity: 2.5445\n",
            "Epoch [3/10], Step [1984/2500], Loss: 1.6231, Perplexity: 5.0685\n",
            "Epoch [3/10], Step [2016/2500], Loss: 0.8908, Perplexity: 2.4372\n",
            "Epoch [3/10], Step [2048/2500], Loss: 1.0552, Perplexity: 2.8724\n",
            "Epoch [3/10], Step [2080/2500], Loss: 1.3420, Perplexity: 3.8266\n",
            "Epoch [3/10], Step [2112/2500], Loss: 1.4218, Perplexity: 4.1445\n",
            "Epoch [3/10], Step [2144/2500], Loss: 1.1464, Perplexity: 3.1468\n",
            "Epoch [3/10], Step [2176/2500], Loss: 1.0946, Perplexity: 2.9879\n",
            "Epoch [3/10], Step [2208/2500], Loss: 1.3296, Perplexity: 3.7794\n",
            "Epoch [3/10], Step [2240/2500], Loss: 0.7905, Perplexity: 2.2046\n",
            "Epoch [3/10], Step [2272/2500], Loss: 1.2821, Perplexity: 3.6041\n",
            "Epoch [3/10], Step [2304/2500], Loss: 1.1299, Perplexity: 3.0953\n",
            "Epoch [3/10], Step [2336/2500], Loss: 1.5632, Perplexity: 4.7739\n",
            "Epoch [3/10], Step [2368/2500], Loss: 1.0863, Perplexity: 2.9633\n",
            "Epoch [3/10], Step [2400/2500], Loss: 1.3952, Perplexity: 4.0359\n",
            "Epoch [3/10], Step [2432/2500], Loss: 1.2218, Perplexity: 3.3934\n",
            "Epoch [3/10], Step [2464/2500], Loss: 1.0471, Perplexity: 2.8495\n",
            "Epoch [3/10], Step [2496/2500], Loss: 1.2340, Perplexity: 3.4349\n",
            "Epoch [4/10], Step [0/2500], Loss: 0.8967, Perplexity: 2.4516\n",
            "Epoch [4/10], Step [32/2500], Loss: 1.2649, Perplexity: 3.5428\n",
            "Epoch [4/10], Step [64/2500], Loss: 1.3639, Perplexity: 3.9113\n",
            "Epoch [4/10], Step [96/2500], Loss: 1.1772, Perplexity: 3.2452\n",
            "Epoch [4/10], Step [128/2500], Loss: 1.1174, Perplexity: 3.0569\n",
            "Epoch [4/10], Step [160/2500], Loss: 1.2194, Perplexity: 3.3851\n",
            "Epoch [4/10], Step [192/2500], Loss: 1.4239, Perplexity: 4.1534\n",
            "Epoch [4/10], Step [224/2500], Loss: 1.3575, Perplexity: 3.8866\n",
            "Epoch [4/10], Step [256/2500], Loss: 1.1128, Perplexity: 3.0429\n",
            "Epoch [4/10], Step [288/2500], Loss: 1.8913, Perplexity: 6.6278\n",
            "Epoch [4/10], Step [320/2500], Loss: 1.3705, Perplexity: 3.9373\n",
            "Epoch [4/10], Step [352/2500], Loss: 1.0777, Perplexity: 2.9379\n",
            "Epoch [4/10], Step [384/2500], Loss: 1.0737, Perplexity: 2.9261\n",
            "Epoch [4/10], Step [416/2500], Loss: 1.2972, Perplexity: 3.6589\n",
            "Epoch [4/10], Step [448/2500], Loss: 1.3955, Perplexity: 4.0371\n",
            "Epoch [4/10], Step [480/2500], Loss: 1.1173, Perplexity: 3.0565\n",
            "Epoch [4/10], Step [512/2500], Loss: 1.4126, Perplexity: 4.1065\n",
            "Epoch [4/10], Step [544/2500], Loss: 0.9933, Perplexity: 2.7002\n",
            "Epoch [4/10], Step [576/2500], Loss: 1.2256, Perplexity: 3.4063\n",
            "Epoch [4/10], Step [608/2500], Loss: 1.4489, Perplexity: 4.2582\n",
            "Epoch [4/10], Step [640/2500], Loss: 1.2669, Perplexity: 3.5500\n",
            "Epoch [4/10], Step [672/2500], Loss: 1.5400, Perplexity: 4.6644\n",
            "Epoch [4/10], Step [704/2500], Loss: 1.8867, Perplexity: 6.5973\n",
            "Epoch [4/10], Step [736/2500], Loss: 1.4231, Perplexity: 4.1499\n",
            "Epoch [4/10], Step [768/2500], Loss: 1.3822, Perplexity: 3.9838\n",
            "Epoch [4/10], Step [800/2500], Loss: 0.9641, Perplexity: 2.6223\n",
            "Epoch [4/10], Step [832/2500], Loss: 1.3228, Perplexity: 3.7538\n",
            "Epoch [4/10], Step [864/2500], Loss: 1.2134, Perplexity: 3.3650\n",
            "Epoch [4/10], Step [896/2500], Loss: 1.2255, Perplexity: 3.4060\n",
            "Epoch [4/10], Step [928/2500], Loss: 0.9905, Perplexity: 2.6926\n",
            "Epoch [4/10], Step [960/2500], Loss: 1.1416, Perplexity: 3.1316\n",
            "Epoch [4/10], Step [992/2500], Loss: 1.5663, Perplexity: 4.7890\n",
            "Epoch [4/10], Step [1024/2500], Loss: 1.1151, Perplexity: 3.0497\n",
            "Epoch [4/10], Step [1056/2500], Loss: 0.9734, Perplexity: 2.6470\n",
            "Epoch [4/10], Step [1088/2500], Loss: 1.1093, Perplexity: 3.0323\n",
            "Epoch [4/10], Step [1120/2500], Loss: 1.1786, Perplexity: 3.2499\n",
            "Epoch [4/10], Step [1152/2500], Loss: 0.9612, Perplexity: 2.6148\n",
            "Epoch [4/10], Step [1184/2500], Loss: 1.5441, Perplexity: 4.6839\n",
            "Epoch [4/10], Step [1216/2500], Loss: 1.1825, Perplexity: 3.2626\n",
            "Epoch [4/10], Step [1248/2500], Loss: 1.2396, Perplexity: 3.4543\n",
            "Epoch [4/10], Step [1280/2500], Loss: 1.3589, Perplexity: 3.8920\n",
            "Epoch [4/10], Step [1312/2500], Loss: 1.3146, Perplexity: 3.7233\n",
            "Epoch [4/10], Step [1344/2500], Loss: 1.3617, Perplexity: 3.9028\n",
            "Epoch [4/10], Step [1376/2500], Loss: 1.1576, Perplexity: 3.1823\n",
            "Epoch [4/10], Step [1408/2500], Loss: 1.1306, Perplexity: 3.0976\n",
            "Epoch [4/10], Step [1440/2500], Loss: 1.4689, Perplexity: 4.3443\n",
            "Epoch [4/10], Step [1472/2500], Loss: 1.3517, Perplexity: 3.8639\n",
            "Epoch [4/10], Step [1504/2500], Loss: 0.9038, Perplexity: 2.4689\n",
            "Epoch [4/10], Step [1536/2500], Loss: 0.9548, Perplexity: 2.5981\n",
            "Epoch [4/10], Step [1568/2500], Loss: 0.9873, Perplexity: 2.6840\n",
            "Epoch [4/10], Step [1600/2500], Loss: 1.2693, Perplexity: 3.5584\n",
            "Epoch [4/10], Step [1632/2500], Loss: 1.2197, Perplexity: 3.3862\n",
            "Epoch [4/10], Step [1664/2500], Loss: 1.2198, Perplexity: 3.3865\n",
            "Epoch [4/10], Step [1696/2500], Loss: 1.0988, Perplexity: 3.0006\n",
            "Epoch [4/10], Step [1728/2500], Loss: 1.3206, Perplexity: 3.7458\n",
            "Epoch [4/10], Step [1760/2500], Loss: 0.7342, Perplexity: 2.0839\n",
            "Epoch [4/10], Step [1792/2500], Loss: 1.2815, Perplexity: 3.6019\n",
            "Epoch [4/10], Step [1824/2500], Loss: 1.3654, Perplexity: 3.9173\n",
            "Epoch [4/10], Step [1856/2500], Loss: 1.4003, Perplexity: 4.0565\n",
            "Epoch [4/10], Step [1888/2500], Loss: 1.2148, Perplexity: 3.3697\n",
            "Epoch [4/10], Step [1920/2500], Loss: 1.3022, Perplexity: 3.6772\n",
            "Epoch [4/10], Step [1952/2500], Loss: 1.4337, Perplexity: 4.1943\n",
            "Epoch [4/10], Step [1984/2500], Loss: 1.2263, Perplexity: 3.4087\n",
            "Epoch [4/10], Step [2016/2500], Loss: 1.4312, Perplexity: 4.1839\n",
            "Epoch [4/10], Step [2048/2500], Loss: 1.4017, Perplexity: 4.0621\n",
            "Epoch [4/10], Step [2080/2500], Loss: 1.1287, Perplexity: 3.0917\n",
            "Epoch [4/10], Step [2112/2500], Loss: 1.0839, Perplexity: 2.9563\n",
            "Epoch [4/10], Step [2144/2500], Loss: 1.2831, Perplexity: 3.6078\n",
            "Epoch [4/10], Step [2176/2500], Loss: 1.2075, Perplexity: 3.3452\n",
            "Epoch [4/10], Step [2208/2500], Loss: 1.0499, Perplexity: 2.8573\n",
            "Epoch [4/10], Step [2240/2500], Loss: 1.4847, Perplexity: 4.4137\n",
            "Epoch [4/10], Step [2272/2500], Loss: 0.5696, Perplexity: 1.7675\n",
            "Epoch [4/10], Step [2304/2500], Loss: 1.2906, Perplexity: 3.6350\n",
            "Epoch [4/10], Step [2336/2500], Loss: 1.1229, Perplexity: 3.0739\n",
            "Epoch [4/10], Step [2368/2500], Loss: 1.0272, Perplexity: 2.7932\n",
            "Epoch [4/10], Step [2400/2500], Loss: 1.1171, Perplexity: 3.0560\n",
            "Epoch [4/10], Step [2432/2500], Loss: 1.3744, Perplexity: 3.9525\n",
            "Epoch [4/10], Step [2464/2500], Loss: 1.5037, Perplexity: 4.4982\n",
            "Epoch [4/10], Step [2496/2500], Loss: 0.7937, Perplexity: 2.2115\n",
            "Epoch [5/10], Step [0/2500], Loss: 1.0909, Perplexity: 2.9770\n",
            "Epoch [5/10], Step [32/2500], Loss: 1.3107, Perplexity: 3.7087\n",
            "Epoch [5/10], Step [64/2500], Loss: 1.0523, Perplexity: 2.8643\n",
            "Epoch [5/10], Step [96/2500], Loss: 0.9222, Perplexity: 2.5147\n",
            "Epoch [5/10], Step [128/2500], Loss: 0.9653, Perplexity: 2.6255\n",
            "Epoch [5/10], Step [160/2500], Loss: 1.4249, Perplexity: 4.1573\n",
            "Epoch [5/10], Step [192/2500], Loss: 1.0296, Perplexity: 2.7999\n",
            "Epoch [5/10], Step [224/2500], Loss: 1.3209, Perplexity: 3.7468\n",
            "Epoch [5/10], Step [256/2500], Loss: 0.9572, Perplexity: 2.6044\n",
            "Epoch [5/10], Step [288/2500], Loss: 0.8375, Perplexity: 2.3105\n",
            "Epoch [5/10], Step [320/2500], Loss: 1.2975, Perplexity: 3.6600\n",
            "Epoch [5/10], Step [352/2500], Loss: 0.9070, Perplexity: 2.4770\n",
            "Epoch [5/10], Step [384/2500], Loss: 1.7989, Perplexity: 6.0431\n",
            "Epoch [5/10], Step [416/2500], Loss: 1.5295, Perplexity: 4.6159\n",
            "Epoch [5/10], Step [448/2500], Loss: 1.2560, Perplexity: 3.5112\n",
            "Epoch [5/10], Step [480/2500], Loss: 0.8483, Perplexity: 2.3357\n",
            "Epoch [5/10], Step [512/2500], Loss: 1.5645, Perplexity: 4.7801\n",
            "Epoch [5/10], Step [544/2500], Loss: 1.4287, Perplexity: 4.1734\n",
            "Epoch [5/10], Step [576/2500], Loss: 1.4135, Perplexity: 4.1101\n",
            "Epoch [5/10], Step [608/2500], Loss: 1.0204, Perplexity: 2.7744\n",
            "Epoch [5/10], Step [640/2500], Loss: 1.7112, Perplexity: 5.5355\n",
            "Epoch [5/10], Step [672/2500], Loss: 1.5789, Perplexity: 4.8497\n",
            "Epoch [5/10], Step [704/2500], Loss: 0.8461, Perplexity: 2.3304\n",
            "Epoch [5/10], Step [736/2500], Loss: 1.1096, Perplexity: 3.0330\n",
            "Epoch [5/10], Step [768/2500], Loss: 0.8879, Perplexity: 2.4300\n",
            "Epoch [5/10], Step [800/2500], Loss: 1.2621, Perplexity: 3.5327\n",
            "Epoch [5/10], Step [832/2500], Loss: 1.1257, Perplexity: 3.0823\n",
            "Epoch [5/10], Step [864/2500], Loss: 1.0420, Perplexity: 2.8349\n",
            "Epoch [5/10], Step [896/2500], Loss: 1.2855, Perplexity: 3.6164\n",
            "Epoch [5/10], Step [928/2500], Loss: 1.2243, Perplexity: 3.4018\n",
            "Epoch [5/10], Step [960/2500], Loss: 1.0919, Perplexity: 2.9800\n",
            "Epoch [5/10], Step [992/2500], Loss: 1.3035, Perplexity: 3.6821\n",
            "Epoch [5/10], Step [1024/2500], Loss: 1.2316, Perplexity: 3.4266\n",
            "Epoch [5/10], Step [1056/2500], Loss: 1.6003, Perplexity: 4.9544\n",
            "Epoch [5/10], Step [1088/2500], Loss: 0.9695, Perplexity: 2.6366\n",
            "Epoch [5/10], Step [1120/2500], Loss: 1.7588, Perplexity: 5.8057\n",
            "Epoch [5/10], Step [1152/2500], Loss: 1.4274, Perplexity: 4.1677\n",
            "Epoch [5/10], Step [1184/2500], Loss: 0.9835, Perplexity: 2.6739\n",
            "Epoch [5/10], Step [1216/2500], Loss: 1.5355, Perplexity: 4.6436\n",
            "Epoch [5/10], Step [1248/2500], Loss: 0.9347, Perplexity: 2.5465\n",
            "Epoch [5/10], Step [1280/2500], Loss: 1.1747, Perplexity: 3.2371\n",
            "Epoch [5/10], Step [1312/2500], Loss: 0.7858, Perplexity: 2.1942\n",
            "Epoch [5/10], Step [1344/2500], Loss: 1.6397, Perplexity: 5.1535\n",
            "Epoch [5/10], Step [1376/2500], Loss: 0.9159, Perplexity: 2.4990\n",
            "Epoch [5/10], Step [1408/2500], Loss: 1.5081, Perplexity: 4.5183\n",
            "Epoch [5/10], Step [1440/2500], Loss: 1.4751, Perplexity: 4.3716\n",
            "Epoch [5/10], Step [1472/2500], Loss: 1.0464, Perplexity: 2.8474\n",
            "Epoch [5/10], Step [1504/2500], Loss: 1.4055, Perplexity: 4.0774\n",
            "Epoch [5/10], Step [1536/2500], Loss: 1.0397, Perplexity: 2.8283\n",
            "Epoch [5/10], Step [1568/2500], Loss: 0.7877, Perplexity: 2.1983\n",
            "Epoch [5/10], Step [1600/2500], Loss: 1.2814, Perplexity: 3.6018\n",
            "Epoch [5/10], Step [1632/2500], Loss: 1.3763, Perplexity: 3.9603\n",
            "Epoch [5/10], Step [1664/2500], Loss: 1.0570, Perplexity: 2.8778\n",
            "Epoch [5/10], Step [1696/2500], Loss: 1.3169, Perplexity: 3.7319\n",
            "Epoch [5/10], Step [1728/2500], Loss: 0.9565, Perplexity: 2.6026\n",
            "Epoch [5/10], Step [1760/2500], Loss: 1.0439, Perplexity: 2.8404\n",
            "Epoch [5/10], Step [1792/2500], Loss: 1.3798, Perplexity: 3.9741\n",
            "Epoch [5/10], Step [1824/2500], Loss: 1.0628, Perplexity: 2.8945\n",
            "Epoch [5/10], Step [1856/2500], Loss: 0.8935, Perplexity: 2.4437\n",
            "Epoch [5/10], Step [1888/2500], Loss: 0.8135, Perplexity: 2.2558\n",
            "Epoch [5/10], Step [1920/2500], Loss: 0.8808, Perplexity: 2.4129\n",
            "Epoch [5/10], Step [1952/2500], Loss: 1.0492, Perplexity: 2.8553\n",
            "Epoch [5/10], Step [1984/2500], Loss: 1.1992, Perplexity: 3.3175\n",
            "Epoch [5/10], Step [2016/2500], Loss: 1.0225, Perplexity: 2.7802\n",
            "Epoch [5/10], Step [2048/2500], Loss: 0.8728, Perplexity: 2.3935\n",
            "Epoch [5/10], Step [2080/2500], Loss: 0.7933, Perplexity: 2.2107\n",
            "Epoch [5/10], Step [2112/2500], Loss: 1.0109, Perplexity: 2.7482\n",
            "Epoch [5/10], Step [2144/2500], Loss: 1.1107, Perplexity: 3.0366\n",
            "Epoch [5/10], Step [2176/2500], Loss: 1.5689, Perplexity: 4.8015\n",
            "Epoch [5/10], Step [2208/2500], Loss: 0.8381, Perplexity: 2.3119\n",
            "Epoch [5/10], Step [2240/2500], Loss: 1.3580, Perplexity: 3.8882\n",
            "Epoch [5/10], Step [2272/2500], Loss: 1.3412, Perplexity: 3.8237\n",
            "Epoch [5/10], Step [2304/2500], Loss: 1.5512, Perplexity: 4.7170\n",
            "Epoch [5/10], Step [2336/2500], Loss: 1.1429, Perplexity: 3.1359\n",
            "Epoch [5/10], Step [2368/2500], Loss: 1.2934, Perplexity: 3.6452\n",
            "Epoch [5/10], Step [2400/2500], Loss: 1.2537, Perplexity: 3.5031\n",
            "Epoch [5/10], Step [2432/2500], Loss: 1.1752, Perplexity: 3.2389\n",
            "Epoch [5/10], Step [2464/2500], Loss: 1.3477, Perplexity: 3.8486\n",
            "Epoch [5/10], Step [2496/2500], Loss: 1.1319, Perplexity: 3.1017\n",
            "Epoch [6/10], Step [0/2500], Loss: 0.8810, Perplexity: 2.4133\n",
            "Epoch [6/10], Step [32/2500], Loss: 0.9732, Perplexity: 2.6463\n",
            "Epoch [6/10], Step [64/2500], Loss: 1.0346, Perplexity: 2.8140\n",
            "Epoch [6/10], Step [96/2500], Loss: 1.1796, Perplexity: 3.2532\n",
            "Epoch [6/10], Step [128/2500], Loss: 1.1604, Perplexity: 3.1911\n",
            "Epoch [6/10], Step [160/2500], Loss: 0.7350, Perplexity: 2.0855\n",
            "Epoch [6/10], Step [192/2500], Loss: 1.4586, Perplexity: 4.3001\n",
            "Epoch [6/10], Step [224/2500], Loss: 1.4333, Perplexity: 4.1927\n",
            "Epoch [6/10], Step [256/2500], Loss: 1.2282, Perplexity: 3.4150\n",
            "Epoch [6/10], Step [288/2500], Loss: 1.4063, Perplexity: 4.0807\n",
            "Epoch [6/10], Step [320/2500], Loss: 1.0013, Perplexity: 2.7219\n",
            "Epoch [6/10], Step [352/2500], Loss: 1.0426, Perplexity: 2.8366\n",
            "Epoch [6/10], Step [384/2500], Loss: 1.5707, Perplexity: 4.8099\n",
            "Epoch [6/10], Step [416/2500], Loss: 0.7320, Perplexity: 2.0793\n",
            "Epoch [6/10], Step [448/2500], Loss: 0.8993, Perplexity: 2.4578\n",
            "Epoch [6/10], Step [480/2500], Loss: 1.3023, Perplexity: 3.6778\n",
            "Epoch [6/10], Step [512/2500], Loss: 0.9530, Perplexity: 2.5935\n",
            "Epoch [6/10], Step [544/2500], Loss: 1.0917, Perplexity: 2.9794\n",
            "Epoch [6/10], Step [576/2500], Loss: 0.7576, Perplexity: 2.1332\n",
            "Epoch [6/10], Step [608/2500], Loss: 1.1083, Perplexity: 3.0292\n",
            "Epoch [6/10], Step [640/2500], Loss: 1.0072, Perplexity: 2.7378\n",
            "Epoch [6/10], Step [672/2500], Loss: 0.7842, Perplexity: 2.1907\n",
            "Epoch [6/10], Step [704/2500], Loss: 1.1338, Perplexity: 3.1075\n",
            "Epoch [6/10], Step [736/2500], Loss: 1.2489, Perplexity: 3.4865\n",
            "Epoch [6/10], Step [768/2500], Loss: 0.9691, Perplexity: 2.6355\n",
            "Epoch [6/10], Step [800/2500], Loss: 0.7512, Perplexity: 2.1194\n",
            "Epoch [6/10], Step [832/2500], Loss: 1.3511, Perplexity: 3.8615\n",
            "Epoch [6/10], Step [864/2500], Loss: 1.1674, Perplexity: 3.2136\n",
            "Epoch [6/10], Step [896/2500], Loss: 1.2627, Perplexity: 3.5351\n",
            "Epoch [6/10], Step [928/2500], Loss: 0.6850, Perplexity: 1.9837\n",
            "Epoch [6/10], Step [960/2500], Loss: 1.1923, Perplexity: 3.2945\n",
            "Epoch [6/10], Step [992/2500], Loss: 1.0509, Perplexity: 2.8602\n",
            "Epoch [6/10], Step [1024/2500], Loss: 1.2445, Perplexity: 3.4712\n",
            "Epoch [6/10], Step [1056/2500], Loss: 1.0563, Perplexity: 2.8757\n",
            "Epoch [6/10], Step [1088/2500], Loss: 0.9993, Perplexity: 2.7165\n",
            "Epoch [6/10], Step [1120/2500], Loss: 1.1201, Perplexity: 3.0652\n",
            "Epoch [6/10], Step [1152/2500], Loss: 1.0043, Perplexity: 2.7300\n",
            "Epoch [6/10], Step [1184/2500], Loss: 1.0224, Perplexity: 2.7798\n",
            "Epoch [6/10], Step [1216/2500], Loss: 1.1260, Perplexity: 3.0833\n",
            "Epoch [6/10], Step [1248/2500], Loss: 1.1963, Perplexity: 3.3078\n",
            "Epoch [6/10], Step [1280/2500], Loss: 1.3475, Perplexity: 3.8479\n",
            "Epoch [6/10], Step [1312/2500], Loss: 1.1702, Perplexity: 3.2226\n",
            "Epoch [6/10], Step [1344/2500], Loss: 1.2940, Perplexity: 3.6474\n",
            "Epoch [6/10], Step [1376/2500], Loss: 0.7801, Perplexity: 2.1816\n",
            "Epoch [6/10], Step [1408/2500], Loss: 1.1005, Perplexity: 3.0056\n",
            "Epoch [6/10], Step [1440/2500], Loss: 1.0471, Perplexity: 2.8495\n",
            "Epoch [6/10], Step [1472/2500], Loss: 1.2320, Perplexity: 3.4282\n",
            "Epoch [6/10], Step [1504/2500], Loss: 1.1096, Perplexity: 3.0333\n",
            "Epoch [6/10], Step [1536/2500], Loss: 1.2163, Perplexity: 3.3748\n",
            "Epoch [6/10], Step [1568/2500], Loss: 1.0250, Perplexity: 2.7871\n",
            "Epoch [6/10], Step [1600/2500], Loss: 1.0607, Perplexity: 2.8885\n",
            "Epoch [6/10], Step [1632/2500], Loss: 1.0589, Perplexity: 2.8833\n",
            "Epoch [6/10], Step [1664/2500], Loss: 0.9619, Perplexity: 2.6167\n",
            "Epoch [6/10], Step [1696/2500], Loss: 1.0121, Perplexity: 2.7514\n",
            "Epoch [6/10], Step [1728/2500], Loss: 0.8672, Perplexity: 2.3804\n",
            "Epoch [6/10], Step [1760/2500], Loss: 0.9499, Perplexity: 2.5856\n",
            "Epoch [6/10], Step [1792/2500], Loss: 1.1695, Perplexity: 3.2202\n",
            "Epoch [6/10], Step [1824/2500], Loss: 0.6943, Perplexity: 2.0022\n",
            "Epoch [6/10], Step [1856/2500], Loss: 1.3811, Perplexity: 3.9793\n",
            "Epoch [6/10], Step [1888/2500], Loss: 1.1066, Perplexity: 3.0240\n",
            "Epoch [6/10], Step [1920/2500], Loss: 0.8716, Perplexity: 2.3908\n",
            "Epoch [6/10], Step [1952/2500], Loss: 1.0013, Perplexity: 2.7219\n",
            "Epoch [6/10], Step [1984/2500], Loss: 1.1280, Perplexity: 3.0895\n",
            "Epoch [6/10], Step [2016/2500], Loss: 1.2111, Perplexity: 3.3572\n",
            "Epoch [6/10], Step [2048/2500], Loss: 0.9022, Perplexity: 2.4649\n",
            "Epoch [6/10], Step [2080/2500], Loss: 0.7567, Perplexity: 2.1311\n",
            "Epoch [6/10], Step [2112/2500], Loss: 1.2707, Perplexity: 3.5632\n",
            "Epoch [6/10], Step [2144/2500], Loss: 1.2270, Perplexity: 3.4111\n",
            "Epoch [6/10], Step [2176/2500], Loss: 1.6735, Perplexity: 5.3307\n",
            "Epoch [6/10], Step [2208/2500], Loss: 0.9938, Perplexity: 2.7016\n",
            "Epoch [6/10], Step [2240/2500], Loss: 0.8767, Perplexity: 2.4031\n",
            "Epoch [6/10], Step [2272/2500], Loss: 1.4284, Perplexity: 4.1719\n",
            "Epoch [6/10], Step [2304/2500], Loss: 0.8740, Perplexity: 2.3964\n",
            "Epoch [6/10], Step [2336/2500], Loss: 1.0569, Perplexity: 2.8775\n",
            "Epoch [6/10], Step [2368/2500], Loss: 0.7951, Perplexity: 2.2146\n",
            "Epoch [6/10], Step [2400/2500], Loss: 0.7625, Perplexity: 2.1436\n",
            "Epoch [6/10], Step [2432/2500], Loss: 0.7300, Perplexity: 2.0750\n",
            "Epoch [6/10], Step [2464/2500], Loss: 0.7944, Perplexity: 2.2131\n",
            "Epoch [6/10], Step [2496/2500], Loss: 1.3749, Perplexity: 3.9548\n",
            "Epoch [7/10], Step [0/2500], Loss: 1.4624, Perplexity: 4.3165\n",
            "Epoch [7/10], Step [32/2500], Loss: 1.0129, Perplexity: 2.7534\n",
            "Epoch [7/10], Step [64/2500], Loss: 1.2779, Perplexity: 3.5891\n",
            "Epoch [7/10], Step [96/2500], Loss: 1.0778, Perplexity: 2.9382\n",
            "Epoch [7/10], Step [128/2500], Loss: 1.0934, Perplexity: 2.9843\n",
            "Epoch [7/10], Step [160/2500], Loss: 0.8439, Perplexity: 2.3254\n",
            "Epoch [7/10], Step [192/2500], Loss: 0.9470, Perplexity: 2.5780\n",
            "Epoch [7/10], Step [224/2500], Loss: 1.1305, Perplexity: 3.0972\n",
            "Epoch [7/10], Step [256/2500], Loss: 0.8528, Perplexity: 2.3461\n",
            "Epoch [7/10], Step [288/2500], Loss: 1.0821, Perplexity: 2.9508\n",
            "Epoch [7/10], Step [320/2500], Loss: 0.9731, Perplexity: 2.6462\n",
            "Epoch [7/10], Step [352/2500], Loss: 1.2096, Perplexity: 3.3522\n",
            "Epoch [7/10], Step [384/2500], Loss: 0.7796, Perplexity: 2.1805\n",
            "Epoch [7/10], Step [416/2500], Loss: 1.0373, Perplexity: 2.8216\n",
            "Epoch [7/10], Step [448/2500], Loss: 1.2494, Perplexity: 3.4883\n",
            "Epoch [7/10], Step [480/2500], Loss: 1.0973, Perplexity: 2.9959\n",
            "Epoch [7/10], Step [512/2500], Loss: 1.1963, Perplexity: 3.3078\n",
            "Epoch [7/10], Step [544/2500], Loss: 0.7724, Perplexity: 2.1650\n",
            "Epoch [7/10], Step [576/2500], Loss: 0.7982, Perplexity: 2.2215\n",
            "Epoch [7/10], Step [608/2500], Loss: 0.8789, Perplexity: 2.4083\n",
            "Epoch [7/10], Step [640/2500], Loss: 0.7908, Perplexity: 2.2053\n",
            "Epoch [7/10], Step [672/2500], Loss: 0.8771, Perplexity: 2.4039\n",
            "Epoch [7/10], Step [704/2500], Loss: 1.3265, Perplexity: 3.7678\n",
            "Epoch [7/10], Step [736/2500], Loss: 1.2065, Perplexity: 3.3417\n",
            "Epoch [7/10], Step [768/2500], Loss: 1.2422, Perplexity: 3.4632\n",
            "Epoch [7/10], Step [800/2500], Loss: 1.1344, Perplexity: 3.1092\n",
            "Epoch [7/10], Step [832/2500], Loss: 1.2280, Perplexity: 3.4143\n",
            "Epoch [7/10], Step [864/2500], Loss: 0.9871, Perplexity: 2.6834\n",
            "Epoch [7/10], Step [896/2500], Loss: 0.5806, Perplexity: 1.7871\n",
            "Epoch [7/10], Step [928/2500], Loss: 1.0313, Perplexity: 2.8046\n",
            "Epoch [7/10], Step [960/2500], Loss: 1.2959, Perplexity: 3.6543\n",
            "Epoch [7/10], Step [992/2500], Loss: 1.0073, Perplexity: 2.7381\n",
            "Epoch [7/10], Step [1024/2500], Loss: 1.0105, Perplexity: 2.7469\n",
            "Epoch [7/10], Step [1056/2500], Loss: 1.4742, Perplexity: 4.3676\n",
            "Epoch [7/10], Step [1088/2500], Loss: 1.1233, Perplexity: 3.0751\n",
            "Epoch [7/10], Step [1120/2500], Loss: 0.9829, Perplexity: 2.6722\n",
            "Epoch [7/10], Step [1152/2500], Loss: 0.9270, Perplexity: 2.5269\n",
            "Epoch [7/10], Step [1184/2500], Loss: 1.0016, Perplexity: 2.7226\n",
            "Epoch [7/10], Step [1216/2500], Loss: 0.9123, Perplexity: 2.4900\n",
            "Epoch [7/10], Step [1248/2500], Loss: 0.7456, Perplexity: 2.1077\n",
            "Epoch [7/10], Step [1280/2500], Loss: 1.0668, Perplexity: 2.9062\n",
            "Epoch [7/10], Step [1312/2500], Loss: 1.0425, Perplexity: 2.8364\n",
            "Epoch [7/10], Step [1344/2500], Loss: 1.1960, Perplexity: 3.3069\n",
            "Epoch [7/10], Step [1376/2500], Loss: 1.0170, Perplexity: 2.7648\n",
            "Epoch [7/10], Step [1408/2500], Loss: 1.3218, Perplexity: 3.7501\n",
            "Epoch [7/10], Step [1440/2500], Loss: 0.9586, Perplexity: 2.6081\n",
            "Epoch [7/10], Step [1472/2500], Loss: 0.7824, Perplexity: 2.1867\n",
            "Epoch [7/10], Step [1504/2500], Loss: 1.1479, Perplexity: 3.1514\n",
            "Epoch [7/10], Step [1536/2500], Loss: 1.2037, Perplexity: 3.3324\n",
            "Epoch [7/10], Step [1568/2500], Loss: 0.8972, Perplexity: 2.4527\n",
            "Epoch [7/10], Step [1600/2500], Loss: 1.0930, Perplexity: 2.9832\n",
            "Epoch [7/10], Step [1632/2500], Loss: 1.1878, Perplexity: 3.2800\n",
            "Epoch [7/10], Step [1664/2500], Loss: 1.3008, Perplexity: 3.6722\n",
            "Epoch [7/10], Step [1696/2500], Loss: 0.9229, Perplexity: 2.5166\n",
            "Epoch [7/10], Step [1728/2500], Loss: 1.3241, Perplexity: 3.7587\n",
            "Epoch [7/10], Step [1760/2500], Loss: 0.8988, Perplexity: 2.4566\n",
            "Epoch [7/10], Step [1792/2500], Loss: 1.0365, Perplexity: 2.8194\n",
            "Epoch [7/10], Step [1824/2500], Loss: 1.0141, Perplexity: 2.7570\n",
            "Epoch [7/10], Step [1856/2500], Loss: 1.1279, Perplexity: 3.0891\n",
            "Epoch [7/10], Step [1888/2500], Loss: 0.9251, Perplexity: 2.5222\n",
            "Epoch [7/10], Step [1920/2500], Loss: 1.0836, Perplexity: 2.9554\n",
            "Epoch [7/10], Step [1952/2500], Loss: 1.2046, Perplexity: 3.3356\n",
            "Epoch [7/10], Step [1984/2500], Loss: 1.0711, Perplexity: 2.9186\n",
            "Epoch [7/10], Step [2016/2500], Loss: 1.0403, Perplexity: 2.8300\n",
            "Epoch [7/10], Step [2048/2500], Loss: 0.9337, Perplexity: 2.5440\n",
            "Epoch [7/10], Step [2080/2500], Loss: 0.8848, Perplexity: 2.4226\n",
            "Epoch [7/10], Step [2112/2500], Loss: 1.0603, Perplexity: 2.8871\n",
            "Epoch [7/10], Step [2144/2500], Loss: 1.1356, Perplexity: 3.1129\n",
            "Epoch [7/10], Step [2176/2500], Loss: 1.0756, Perplexity: 2.9319\n",
            "Epoch [7/10], Step [2208/2500], Loss: 1.0917, Perplexity: 2.9794\n",
            "Epoch [7/10], Step [2240/2500], Loss: 1.0958, Perplexity: 2.9915\n",
            "Epoch [7/10], Step [2272/2500], Loss: 1.5371, Perplexity: 4.6511\n",
            "Epoch [7/10], Step [2304/2500], Loss: 1.0409, Perplexity: 2.8319\n",
            "Epoch [7/10], Step [2336/2500], Loss: 1.0819, Perplexity: 2.9502\n",
            "Epoch [7/10], Step [2368/2500], Loss: 0.9265, Perplexity: 2.5256\n",
            "Epoch [7/10], Step [2400/2500], Loss: 0.9951, Perplexity: 2.7050\n",
            "Epoch [7/10], Step [2432/2500], Loss: 0.7414, Perplexity: 2.0989\n",
            "Epoch [7/10], Step [2464/2500], Loss: 1.1335, Perplexity: 3.1066\n",
            "Epoch [7/10], Step [2496/2500], Loss: 0.8942, Perplexity: 2.4454\n",
            "Epoch [8/10], Step [0/2500], Loss: 1.2625, Perplexity: 3.5342\n",
            "Epoch [8/10], Step [32/2500], Loss: 1.4156, Perplexity: 4.1189\n",
            "Epoch [8/10], Step [64/2500], Loss: 0.8871, Perplexity: 2.4281\n",
            "Epoch [8/10], Step [96/2500], Loss: 0.9942, Perplexity: 2.7027\n",
            "Epoch [8/10], Step [128/2500], Loss: 1.3467, Perplexity: 3.8446\n",
            "Epoch [8/10], Step [160/2500], Loss: 1.0376, Perplexity: 2.8224\n",
            "Epoch [8/10], Step [192/2500], Loss: 1.1659, Perplexity: 3.2087\n",
            "Epoch [8/10], Step [224/2500], Loss: 0.9120, Perplexity: 2.4893\n",
            "Epoch [8/10], Step [256/2500], Loss: 1.0763, Perplexity: 2.9337\n",
            "Epoch [8/10], Step [288/2500], Loss: 0.9987, Perplexity: 2.7148\n",
            "Epoch [8/10], Step [320/2500], Loss: 1.0387, Perplexity: 2.8257\n",
            "Epoch [8/10], Step [352/2500], Loss: 1.2912, Perplexity: 3.6370\n",
            "Epoch [8/10], Step [384/2500], Loss: 1.0677, Perplexity: 2.9087\n",
            "Epoch [8/10], Step [416/2500], Loss: 1.1201, Perplexity: 3.0652\n",
            "Epoch [8/10], Step [448/2500], Loss: 1.2261, Perplexity: 3.4079\n",
            "Epoch [8/10], Step [480/2500], Loss: 0.7452, Perplexity: 2.1069\n",
            "Epoch [8/10], Step [512/2500], Loss: 0.7446, Perplexity: 2.1056\n",
            "Epoch [8/10], Step [544/2500], Loss: 0.9028, Perplexity: 2.4665\n",
            "Epoch [8/10], Step [576/2500], Loss: 0.9454, Perplexity: 2.5739\n",
            "Epoch [8/10], Step [608/2500], Loss: 1.2198, Perplexity: 3.3863\n",
            "Epoch [8/10], Step [640/2500], Loss: 1.1281, Perplexity: 3.0898\n",
            "Epoch [8/10], Step [672/2500], Loss: 0.8167, Perplexity: 2.2630\n",
            "Epoch [8/10], Step [704/2500], Loss: 0.6447, Perplexity: 1.9054\n",
            "Epoch [8/10], Step [736/2500], Loss: 0.8092, Perplexity: 2.2461\n",
            "Epoch [8/10], Step [768/2500], Loss: 1.2835, Perplexity: 3.6092\n",
            "Epoch [8/10], Step [800/2500], Loss: 1.2911, Perplexity: 3.6369\n",
            "Epoch [8/10], Step [832/2500], Loss: 1.2310, Perplexity: 3.4247\n",
            "Epoch [8/10], Step [864/2500], Loss: 0.6466, Perplexity: 1.9091\n",
            "Epoch [8/10], Step [896/2500], Loss: 1.0430, Perplexity: 2.8377\n",
            "Epoch [8/10], Step [928/2500], Loss: 1.6071, Perplexity: 4.9884\n",
            "Epoch [8/10], Step [960/2500], Loss: 1.2831, Perplexity: 3.6079\n",
            "Epoch [8/10], Step [992/2500], Loss: 1.5044, Perplexity: 4.5014\n",
            "Epoch [8/10], Step [1024/2500], Loss: 0.9069, Perplexity: 2.4765\n",
            "Epoch [8/10], Step [1056/2500], Loss: 0.9855, Perplexity: 2.6792\n",
            "Epoch [8/10], Step [1088/2500], Loss: 1.2595, Perplexity: 3.5236\n",
            "Epoch [8/10], Step [1120/2500], Loss: 1.0869, Perplexity: 2.9650\n",
            "Epoch [8/10], Step [1152/2500], Loss: 0.9401, Perplexity: 2.5603\n",
            "Epoch [8/10], Step [1184/2500], Loss: 0.7192, Perplexity: 2.0528\n",
            "Epoch [8/10], Step [1216/2500], Loss: 1.0653, Perplexity: 2.9017\n",
            "Epoch [8/10], Step [1248/2500], Loss: 1.4191, Perplexity: 4.1335\n",
            "Epoch [8/10], Step [1280/2500], Loss: 1.3945, Perplexity: 4.0329\n",
            "Epoch [8/10], Step [1312/2500], Loss: 0.6960, Perplexity: 2.0057\n",
            "Epoch [8/10], Step [1344/2500], Loss: 1.2676, Perplexity: 3.5525\n",
            "Epoch [8/10], Step [1376/2500], Loss: 0.8046, Perplexity: 2.2357\n",
            "Epoch [8/10], Step [1408/2500], Loss: 0.8133, Perplexity: 2.2554\n",
            "Epoch [8/10], Step [1440/2500], Loss: 0.7215, Perplexity: 2.0574\n",
            "Epoch [8/10], Step [1472/2500], Loss: 1.2048, Perplexity: 3.3362\n",
            "Epoch [8/10], Step [1504/2500], Loss: 1.2132, Perplexity: 3.3642\n",
            "Epoch [8/10], Step [1536/2500], Loss: 0.7851, Perplexity: 2.1927\n",
            "Epoch [8/10], Step [1568/2500], Loss: 0.7878, Perplexity: 2.1985\n",
            "Epoch [8/10], Step [1600/2500], Loss: 1.0623, Perplexity: 2.8931\n",
            "Epoch [8/10], Step [1632/2500], Loss: 0.7317, Perplexity: 2.0786\n",
            "Epoch [8/10], Step [1664/2500], Loss: 0.3861, Perplexity: 1.4713\n",
            "Epoch [8/10], Step [1696/2500], Loss: 1.4450, Perplexity: 4.2418\n",
            "Epoch [8/10], Step [1728/2500], Loss: 0.9804, Perplexity: 2.6656\n",
            "Epoch [8/10], Step [1760/2500], Loss: 1.0172, Perplexity: 2.7653\n",
            "Epoch [8/10], Step [1792/2500], Loss: 0.8146, Perplexity: 2.2583\n",
            "Epoch [8/10], Step [1824/2500], Loss: 0.6874, Perplexity: 1.9885\n",
            "Epoch [8/10], Step [1856/2500], Loss: 1.1425, Perplexity: 3.1346\n",
            "Epoch [8/10], Step [1888/2500], Loss: 0.9450, Perplexity: 2.5729\n",
            "Epoch [8/10], Step [1920/2500], Loss: 1.0383, Perplexity: 2.8245\n",
            "Epoch [8/10], Step [1952/2500], Loss: 0.6067, Perplexity: 1.8343\n",
            "Epoch [8/10], Step [1984/2500], Loss: 0.7703, Perplexity: 2.1605\n",
            "Epoch [8/10], Step [2016/2500], Loss: 1.2904, Perplexity: 3.6342\n",
            "Epoch [8/10], Step [2048/2500], Loss: 1.4921, Perplexity: 4.4464\n",
            "Epoch [8/10], Step [2080/2500], Loss: 1.3509, Perplexity: 3.8610\n",
            "Epoch [8/10], Step [2112/2500], Loss: 1.5796, Perplexity: 4.8529\n",
            "Epoch [8/10], Step [2144/2500], Loss: 1.1183, Perplexity: 3.0597\n",
            "Epoch [8/10], Step [2176/2500], Loss: 1.1076, Perplexity: 3.0272\n",
            "Epoch [8/10], Step [2208/2500], Loss: 0.7399, Perplexity: 2.0956\n",
            "Epoch [8/10], Step [2240/2500], Loss: 1.3354, Perplexity: 3.8014\n",
            "Epoch [8/10], Step [2272/2500], Loss: 0.9397, Perplexity: 2.5591\n",
            "Epoch [8/10], Step [2304/2500], Loss: 0.8118, Perplexity: 2.2519\n",
            "Epoch [8/10], Step [2336/2500], Loss: 0.6274, Perplexity: 1.8728\n",
            "Epoch [8/10], Step [2368/2500], Loss: 1.0055, Perplexity: 2.7333\n",
            "Epoch [8/10], Step [2400/2500], Loss: 1.0658, Perplexity: 2.9033\n",
            "Epoch [8/10], Step [2432/2500], Loss: 0.8084, Perplexity: 2.2444\n",
            "Epoch [8/10], Step [2464/2500], Loss: 0.7539, Perplexity: 2.1252\n",
            "Epoch [8/10], Step [2496/2500], Loss: 0.9289, Perplexity: 2.5317\n",
            "Epoch [9/10], Step [0/2500], Loss: 0.9041, Perplexity: 2.4698\n",
            "Epoch [9/10], Step [32/2500], Loss: 0.8496, Perplexity: 2.3388\n",
            "Epoch [9/10], Step [64/2500], Loss: 0.9507, Perplexity: 2.5875\n",
            "Epoch [9/10], Step [96/2500], Loss: 0.7676, Perplexity: 2.1547\n",
            "Epoch [9/10], Step [128/2500], Loss: 1.0533, Perplexity: 2.8672\n",
            "Epoch [9/10], Step [160/2500], Loss: 1.1608, Perplexity: 3.1926\n",
            "Epoch [9/10], Step [192/2500], Loss: 0.7316, Perplexity: 2.0785\n",
            "Epoch [9/10], Step [224/2500], Loss: 0.7246, Perplexity: 2.0639\n",
            "Epoch [9/10], Step [256/2500], Loss: 0.6950, Perplexity: 2.0036\n",
            "Epoch [9/10], Step [288/2500], Loss: 1.1178, Perplexity: 3.0582\n",
            "Epoch [9/10], Step [320/2500], Loss: 0.6236, Perplexity: 1.8656\n",
            "Epoch [9/10], Step [352/2500], Loss: 0.8589, Perplexity: 2.3606\n",
            "Epoch [9/10], Step [384/2500], Loss: 0.8141, Perplexity: 2.2572\n",
            "Epoch [9/10], Step [416/2500], Loss: 0.7318, Perplexity: 2.0787\n",
            "Epoch [9/10], Step [448/2500], Loss: 1.0498, Perplexity: 2.8570\n",
            "Epoch [9/10], Step [480/2500], Loss: 0.9079, Perplexity: 2.4791\n",
            "Epoch [9/10], Step [512/2500], Loss: 0.6778, Perplexity: 1.9695\n",
            "Epoch [9/10], Step [544/2500], Loss: 0.6054, Perplexity: 1.8320\n",
            "Epoch [9/10], Step [576/2500], Loss: 1.0550, Perplexity: 2.8720\n",
            "Epoch [9/10], Step [608/2500], Loss: 0.8694, Perplexity: 2.3855\n",
            "Epoch [9/10], Step [640/2500], Loss: 1.5483, Perplexity: 4.7034\n",
            "Epoch [9/10], Step [672/2500], Loss: 0.7519, Perplexity: 2.1210\n",
            "Epoch [9/10], Step [704/2500], Loss: 0.8346, Perplexity: 2.3038\n",
            "Epoch [9/10], Step [736/2500], Loss: 1.0666, Perplexity: 2.9056\n",
            "Epoch [9/10], Step [768/2500], Loss: 0.8676, Perplexity: 2.3811\n",
            "Epoch [9/10], Step [800/2500], Loss: 0.8708, Perplexity: 2.3889\n",
            "Epoch [9/10], Step [832/2500], Loss: 0.9607, Perplexity: 2.6135\n",
            "Epoch [9/10], Step [864/2500], Loss: 1.1794, Perplexity: 3.2524\n",
            "Epoch [9/10], Step [896/2500], Loss: 0.8549, Perplexity: 2.3511\n",
            "Epoch [9/10], Step [928/2500], Loss: 1.0965, Perplexity: 2.9937\n",
            "Epoch [9/10], Step [960/2500], Loss: 0.9188, Perplexity: 2.5062\n",
            "Epoch [9/10], Step [992/2500], Loss: 0.9675, Perplexity: 2.6314\n",
            "Epoch [9/10], Step [1024/2500], Loss: 1.6271, Perplexity: 5.0892\n",
            "Epoch [9/10], Step [1056/2500], Loss: 0.9932, Perplexity: 2.6998\n",
            "Epoch [9/10], Step [1088/2500], Loss: 1.0979, Perplexity: 2.9978\n",
            "Epoch [9/10], Step [1120/2500], Loss: 1.2936, Perplexity: 3.6461\n",
            "Epoch [9/10], Step [1152/2500], Loss: 0.6914, Perplexity: 1.9965\n",
            "Epoch [9/10], Step [1184/2500], Loss: 0.7252, Perplexity: 2.0652\n",
            "Epoch [9/10], Step [1216/2500], Loss: 1.0999, Perplexity: 3.0039\n",
            "Epoch [9/10], Step [1248/2500], Loss: 1.1400, Perplexity: 3.1267\n",
            "Epoch [9/10], Step [1280/2500], Loss: 1.2530, Perplexity: 3.5008\n",
            "Epoch [9/10], Step [1312/2500], Loss: 0.9464, Perplexity: 2.5764\n",
            "Epoch [9/10], Step [1344/2500], Loss: 1.0928, Perplexity: 2.9826\n",
            "Epoch [9/10], Step [1376/2500], Loss: 0.9957, Perplexity: 2.7067\n",
            "Epoch [9/10], Step [1408/2500], Loss: 1.0276, Perplexity: 2.7943\n",
            "Epoch [9/10], Step [1440/2500], Loss: 0.8828, Perplexity: 2.4176\n",
            "Epoch [9/10], Step [1472/2500], Loss: 0.7170, Perplexity: 2.0482\n",
            "Epoch [9/10], Step [1504/2500], Loss: 1.1365, Perplexity: 3.1158\n",
            "Epoch [9/10], Step [1536/2500], Loss: 1.3632, Perplexity: 3.9088\n",
            "Epoch [9/10], Step [1568/2500], Loss: 1.0026, Perplexity: 2.7254\n",
            "Epoch [9/10], Step [1600/2500], Loss: 1.0690, Perplexity: 2.9124\n",
            "Epoch [9/10], Step [1632/2500], Loss: 0.9747, Perplexity: 2.6505\n",
            "Epoch [9/10], Step [1664/2500], Loss: 1.1839, Perplexity: 3.2672\n",
            "Epoch [9/10], Step [1696/2500], Loss: 1.3867, Perplexity: 4.0014\n",
            "Epoch [9/10], Step [1728/2500], Loss: 1.2588, Perplexity: 3.5211\n",
            "Epoch [9/10], Step [1760/2500], Loss: 1.3554, Perplexity: 3.8785\n",
            "Epoch [9/10], Step [1792/2500], Loss: 0.8389, Perplexity: 2.3138\n",
            "Epoch [9/10], Step [1824/2500], Loss: 0.8996, Perplexity: 2.4587\n",
            "Epoch [9/10], Step [1856/2500], Loss: 0.8193, Perplexity: 2.2690\n",
            "Epoch [9/10], Step [1888/2500], Loss: 0.6477, Perplexity: 1.9112\n",
            "Epoch [9/10], Step [1920/2500], Loss: 0.8972, Perplexity: 2.4526\n",
            "Epoch [9/10], Step [1952/2500], Loss: 1.1785, Perplexity: 3.2495\n",
            "Epoch [9/10], Step [1984/2500], Loss: 1.2609, Perplexity: 3.5284\n",
            "Epoch [9/10], Step [2016/2500], Loss: 1.0451, Perplexity: 2.8436\n",
            "Epoch [9/10], Step [2048/2500], Loss: 1.1265, Perplexity: 3.0849\n",
            "Epoch [9/10], Step [2080/2500], Loss: 1.2221, Perplexity: 3.3943\n",
            "Epoch [9/10], Step [2112/2500], Loss: 0.7269, Perplexity: 2.0686\n",
            "Epoch [9/10], Step [2144/2500], Loss: 1.0564, Perplexity: 2.8761\n",
            "Epoch [9/10], Step [2176/2500], Loss: 0.8388, Perplexity: 2.3136\n",
            "Epoch [9/10], Step [2208/2500], Loss: 0.7188, Perplexity: 2.0520\n",
            "Epoch [9/10], Step [2240/2500], Loss: 1.0640, Perplexity: 2.8978\n",
            "Epoch [9/10], Step [2272/2500], Loss: 0.9389, Perplexity: 2.5573\n",
            "Epoch [9/10], Step [2304/2500], Loss: 0.9949, Perplexity: 2.7045\n",
            "Epoch [9/10], Step [2336/2500], Loss: 0.9875, Perplexity: 2.6846\n",
            "Epoch [9/10], Step [2368/2500], Loss: 1.0077, Perplexity: 2.7393\n",
            "Epoch [9/10], Step [2400/2500], Loss: 0.6768, Perplexity: 1.9675\n",
            "Epoch [9/10], Step [2432/2500], Loss: 0.9299, Perplexity: 2.5343\n",
            "Epoch [9/10], Step [2464/2500], Loss: 0.6905, Perplexity: 1.9947\n",
            "Epoch [9/10], Step [2496/2500], Loss: 1.0570, Perplexity: 2.8776\n",
            "Epoch [10/10], Step [0/2500], Loss: 1.1951, Perplexity: 3.3041\n",
            "Epoch [10/10], Step [32/2500], Loss: 1.1284, Perplexity: 3.0907\n",
            "Epoch [10/10], Step [64/2500], Loss: 1.5106, Perplexity: 4.5296\n",
            "Epoch [10/10], Step [96/2500], Loss: 1.0740, Perplexity: 2.9270\n",
            "Epoch [10/10], Step [128/2500], Loss: 0.9702, Perplexity: 2.6384\n",
            "Epoch [10/10], Step [160/2500], Loss: 0.8718, Perplexity: 2.3911\n",
            "Epoch [10/10], Step [192/2500], Loss: 0.9474, Perplexity: 2.5789\n",
            "Epoch [10/10], Step [224/2500], Loss: 0.6729, Perplexity: 1.9600\n",
            "Epoch [10/10], Step [256/2500], Loss: 1.0259, Perplexity: 2.7895\n",
            "Epoch [10/10], Step [288/2500], Loss: 1.1315, Perplexity: 3.1004\n",
            "Epoch [10/10], Step [320/2500], Loss: 0.6312, Perplexity: 1.8798\n",
            "Epoch [10/10], Step [352/2500], Loss: 0.9076, Perplexity: 2.4783\n",
            "Epoch [10/10], Step [384/2500], Loss: 0.9516, Perplexity: 2.5899\n",
            "Epoch [10/10], Step [416/2500], Loss: 0.8963, Perplexity: 2.4504\n",
            "Epoch [10/10], Step [448/2500], Loss: 0.6950, Perplexity: 2.0037\n",
            "Epoch [10/10], Step [480/2500], Loss: 1.0387, Perplexity: 2.8257\n",
            "Epoch [10/10], Step [512/2500], Loss: 1.0813, Perplexity: 2.9484\n",
            "Epoch [10/10], Step [544/2500], Loss: 1.0571, Perplexity: 2.8779\n",
            "Epoch [10/10], Step [576/2500], Loss: 1.1295, Perplexity: 3.0943\n",
            "Epoch [10/10], Step [608/2500], Loss: 0.7879, Perplexity: 2.1988\n",
            "Epoch [10/10], Step [640/2500], Loss: 1.1617, Perplexity: 3.1954\n",
            "Epoch [10/10], Step [672/2500], Loss: 0.8724, Perplexity: 2.3926\n",
            "Epoch [10/10], Step [704/2500], Loss: 0.6032, Perplexity: 1.8279\n",
            "Epoch [10/10], Step [736/2500], Loss: 1.0005, Perplexity: 2.7195\n",
            "Epoch [10/10], Step [768/2500], Loss: 1.0835, Perplexity: 2.9550\n",
            "Epoch [10/10], Step [800/2500], Loss: 1.0675, Perplexity: 2.9082\n",
            "Epoch [10/10], Step [832/2500], Loss: 0.8841, Perplexity: 2.4209\n",
            "Epoch [10/10], Step [864/2500], Loss: 1.3649, Perplexity: 3.9153\n",
            "Epoch [10/10], Step [896/2500], Loss: 1.0889, Perplexity: 2.9711\n",
            "Epoch [10/10], Step [928/2500], Loss: 0.9477, Perplexity: 2.5799\n",
            "Epoch [10/10], Step [960/2500], Loss: 0.7450, Perplexity: 2.1064\n",
            "Epoch [10/10], Step [992/2500], Loss: 0.8077, Perplexity: 2.2427\n",
            "Epoch [10/10], Step [1024/2500], Loss: 0.5535, Perplexity: 1.7394\n",
            "Epoch [10/10], Step [1056/2500], Loss: 0.9594, Perplexity: 2.6100\n",
            "Epoch [10/10], Step [1088/2500], Loss: 0.6386, Perplexity: 1.8939\n",
            "Epoch [10/10], Step [1120/2500], Loss: 1.0393, Perplexity: 2.8271\n",
            "Epoch [10/10], Step [1152/2500], Loss: 0.8480, Perplexity: 2.3350\n",
            "Epoch [10/10], Step [1184/2500], Loss: 1.0703, Perplexity: 2.9162\n",
            "Epoch [10/10], Step [1216/2500], Loss: 1.0642, Perplexity: 2.8985\n",
            "Epoch [10/10], Step [1248/2500], Loss: 1.1411, Perplexity: 3.1301\n",
            "Epoch [10/10], Step [1280/2500], Loss: 1.2735, Perplexity: 3.5734\n",
            "Epoch [10/10], Step [1312/2500], Loss: 1.0016, Perplexity: 2.7225\n",
            "Epoch [10/10], Step [1344/2500], Loss: 0.8832, Perplexity: 2.4186\n",
            "Epoch [10/10], Step [1376/2500], Loss: 1.0040, Perplexity: 2.7291\n",
            "Epoch [10/10], Step [1408/2500], Loss: 0.6650, Perplexity: 1.9446\n",
            "Epoch [10/10], Step [1440/2500], Loss: 0.7922, Perplexity: 2.2081\n",
            "Epoch [10/10], Step [1472/2500], Loss: 1.1005, Perplexity: 3.0056\n",
            "Epoch [10/10], Step [1504/2500], Loss: 1.2630, Perplexity: 3.5360\n",
            "Epoch [10/10], Step [1536/2500], Loss: 1.4715, Perplexity: 4.3558\n",
            "Epoch [10/10], Step [1568/2500], Loss: 0.9781, Perplexity: 2.6593\n",
            "Epoch [10/10], Step [1600/2500], Loss: 0.7621, Perplexity: 2.1428\n",
            "Epoch [10/10], Step [1632/2500], Loss: 0.6418, Perplexity: 1.8998\n",
            "Epoch [10/10], Step [1664/2500], Loss: 1.1373, Perplexity: 3.1182\n",
            "Epoch [10/10], Step [1696/2500], Loss: 1.0155, Perplexity: 2.7607\n",
            "Epoch [10/10], Step [1728/2500], Loss: 0.7259, Perplexity: 2.0665\n",
            "Epoch [10/10], Step [1760/2500], Loss: 1.1040, Perplexity: 3.0162\n",
            "Epoch [10/10], Step [1792/2500], Loss: 0.8369, Perplexity: 2.3093\n",
            "Epoch [10/10], Step [1824/2500], Loss: 0.8088, Perplexity: 2.2451\n",
            "Epoch [10/10], Step [1856/2500], Loss: 0.9062, Perplexity: 2.4748\n",
            "Epoch [10/10], Step [1888/2500], Loss: 0.6293, Perplexity: 1.8762\n",
            "Epoch [10/10], Step [1920/2500], Loss: 0.8569, Perplexity: 2.3560\n",
            "Epoch [10/10], Step [1952/2500], Loss: 0.8299, Perplexity: 2.2930\n",
            "Epoch [10/10], Step [1984/2500], Loss: 0.7456, Perplexity: 2.1077\n",
            "Epoch [10/10], Step [2016/2500], Loss: 0.8580, Perplexity: 2.3585\n",
            "Epoch [10/10], Step [2048/2500], Loss: 0.8882, Perplexity: 2.4307\n",
            "Epoch [10/10], Step [2080/2500], Loss: 0.7399, Perplexity: 2.0957\n",
            "Epoch [10/10], Step [2112/2500], Loss: 0.9294, Perplexity: 2.5330\n",
            "Epoch [10/10], Step [2144/2500], Loss: 1.1836, Perplexity: 3.2660\n",
            "Epoch [10/10], Step [2176/2500], Loss: 0.8916, Perplexity: 2.4390\n",
            "Epoch [10/10], Step [2208/2500], Loss: 0.9664, Perplexity: 2.6284\n",
            "Epoch [10/10], Step [2240/2500], Loss: 0.7738, Perplexity: 2.1680\n",
            "Epoch [10/10], Step [2272/2500], Loss: 0.5957, Perplexity: 1.8142\n",
            "Epoch [10/10], Step [2304/2500], Loss: 1.0803, Perplexity: 2.9454\n",
            "Epoch [10/10], Step [2336/2500], Loss: 0.8629, Perplexity: 2.3701\n",
            "Epoch [10/10], Step [2368/2500], Loss: 0.9744, Perplexity: 2.6497\n",
            "Epoch [10/10], Step [2400/2500], Loss: 1.2219, Perplexity: 3.3936\n",
            "Epoch [10/10], Step [2432/2500], Loss: 0.7420, Perplexity: 2.1001\n",
            "Epoch [10/10], Step [2464/2500], Loss: 0.8792, Perplexity: 2.4090\n",
            "Epoch [10/10], Step [2496/2500], Loss: 1.4185, Perplexity: 4.1308\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "translations_list = []  # List to store translated sentences\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (images, prompts, trg_lengths) in enumerate(data_loader):\n",
        "        images = images.to(device)\n",
        "        prompts = prompts.to(device)\n",
        "\n",
        "        # TODO add packing?\n",
        "        #targets = pack_padded_sequence(prompts, trg_lengths, batch_first=True)[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images, prompts)\n",
        "\n",
        "        # Remove the <sos> token and reshape the output and target tensors\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim).contiguous()\n",
        "\n",
        "        trg = prompts.transpose(0, 1)[1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % batch_size == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
        "                  .format(epoch + 1, num_epochs, i, len(data_loader), loss.item(), np.exp(loss.item())))\n",
        "\n",
        "    # Get translations after each epoch and append to the list\n",
        "    translations_list.append(get_translations(example_images, example_prompts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'seq2seq.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): EncoderCNN(\n",
              "    (resnet): ResNet(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      (fc): Linear(in_features=2048, out_features=256, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): DecoderRNN(\n",
              "    (embedding): Embedding(10003, 512)\n",
              "    (rnn): LSTM(512, 256, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=256, out_features=10003, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder = EncoderCNN(embed_size, n_layers, hidden_size).to(device)\n",
        "decoder = DecoderRNN(output_size, embed_size, n_layers, hidden_size, dec_dropout).to(device)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "model.load_state_dict(torch.load(\"seq2seq.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YukQyqEGKqyi",
        "outputId": "38b4c0ba-4781-480e-b299-3ded85221ecb"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m translations_first_example, translations_second_example \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mtranslations_list)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTranslated first Prompt:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(translated_example_prompts[\u001b[39m0\u001b[39m])\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "translations_first_example, translations_second_example = zip(*translations_list)\n",
        "\n",
        "print(\"Translated first Prompt:\")\n",
        "print(translated_example_prompts[0])\n",
        "\n",
        "\n",
        "print(\"Translated outputs over epochs:\")\n",
        "print()\n",
        "# Print the translated prompts over epochs\n",
        "for i, sentence in enumerate(translations_first_example):\n",
        "    print(f\"Epoch {i+1} predicted sentence {sentence}\")\n",
        "\n",
        "#print()\n",
        "\n",
        "#print(\"Translated second Prompt:\")\n",
        "#print(translated_example_prompts[0])\n",
        "\n",
        "# Print the translated prompts over epochs\n",
        "#for i, sentence in enumerate(translations_first_example):\n",
        "#    print(f\"Epoch {i+1} predicted sentence {sentence}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVH7yKSx00mg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ptsT7f2P07T-"
      },
      "outputs": [],
      "source": [
        "test_dataset = DiffusionDataset(data_path, word2index_path, train_split_ratio=0.8, train=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "4lf_VRHJ09Xg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output Shape: torch.Size([29, 32, 10003])\n",
            "Output Shape -1: 10003\n",
            "Output View Shape: torch.Size([895, 10003])\n",
            "Output View Shape: torch.Size([895, 10003])\n",
            "Prompts Shape: torch.Size([32, 29])\n",
            "Prompts Transpose and remove sos Shape: torch.Size([28, 32])\n",
            "Prompts Transpose and remove sos and contiguous Shape: torch.Size([28, 32])\n",
            "Prompts Transpose and remove sos and contiguous and view -1 Shape: torch.Size([896])\n",
            "Output: tensor([[  4.4931, -12.7695,  -0.9353,  ..., -12.7515, -12.6313, -12.7571],\n",
            "        [  4.5014, -12.7801,  -0.9704,  ..., -12.7709, -12.6168, -12.7889],\n",
            "        [  4.4376, -12.7442,  -0.8927,  ..., -12.8039, -12.5320, -12.8345],\n",
            "        ...,\n",
            "        [  7.7523, -13.0151,   2.1325,  ..., -13.3175, -13.2672, -12.9812],\n",
            "        [ 16.1035, -20.3290,   0.1951,  ..., -20.2081, -19.7453, -19.6871],\n",
            "        [ 16.1529, -19.9414,   0.2913,  ..., -19.9118, -19.2234, -19.2717]],\n",
            "       device='cuda:0'), Shape: torch.Size([896, 10003])\n",
            "\n",
            "Target: tensor([  17,   17,   17,   17,   17,   17,   17,   17, 2842, 2842, 2842, 2842,\n",
            "         247,  247,  247,  247,  167,  167,  167,  167,  247,  247,  247,  247,\n",
            "          17,  113,  113,  113,  113,    7,  197,  197,    5,    5,    5,    5,\n",
            "           5,    5,    5,    5,    0,    0,    0,    0,   64,   64,   64,   64,\n",
            "         232,  232,  232,  232,   64,   64,   64,   64,    5,  473,  473,  473,\n",
            "         473,  315,   17,   17,    3,    3,    3,    3,    3,    3,    3,    3,\n",
            "           8,    8,    8,    8,   35,   35,   35,   35,   67,   67,   67,   67,\n",
            "          35,   35,   35,   35,  798, 5025, 5025, 5025, 5025,    5,    0,    0,\n",
            "          25,   25,   25,   25,   25,   25,   25,   25,   10,   10,   10,   10,\n",
            "         219,  219,  219,  219,   72,   72,   72,   72,  219,  219,  219,  219,\n",
            "           5,   43,   43,   43,   43,    3, 2163, 2163,  257,  257,  257,  257,\n",
            "         257,  257,  257,  257,  545,  545,  545,  545,  488,  488,  488,  488,\n",
            "          60,   60,   60,   60,   44,   44,   44,   44, 1928,  777,  777,  777,\n",
            "         777,  184, 2164, 2164,   51,   51,   51,   51,   51,   51,   51,   51,\n",
            "           5,    5,    5,    5,    4,    4,    4,    4,    5,    5,    5,    5,\n",
            "          23,   23,   23,   23, 3600,  299,  299,  299,  299, 1329,    2,    2,\n",
            "         537,  537,  537,  537,  754,  754,  754,  754,    0,    0,    0,    0,\n",
            "         532,  532,  532,  532,    3,    3,    3,    3,   13,   13,   13,   13,\n",
            "        1987,    0,    0,    0,    0,    0,    0,    0,  125,  125,  125,  125,\n",
            "          48,   48,   48,   48,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "        2450, 2450, 2450, 2450,  316,  316,  316,  316, 4247,  299,  299,  299,\n",
            "         299,   94,    0,    0,  235,  235,  235,  235,    0,    0,    0,    0,\n",
            "         110,  110,  110,  110, 2424, 2424, 2424, 2424,    6,    6,    6,    6,\n",
            "          15,   15,   15,   15,   69,  848,  848,  848,  848,    0,    0,    0,\n",
            "           5,    5,    5,    5, 8746, 8746, 8746, 8746,   75,   75,   75,   75,\n",
            "          44,   44,   44,   44, 1145, 1145, 1145, 1145, 1016, 1016, 1016, 1016,\n",
            "          12,    2,    2,    2,    2,    2,    0,    0,  754,  754,  754,  754,\n",
            "           6,    6,    6,    6,    0,    0,    0,    0,   23,   23,   23,   23,\n",
            "         193,  193,  193,  193,  933,  933,  933,  933, 3364,    0,    0,    0,\n",
            "           0,    0,    0,    0,   73,   73,   73,   73, 1145, 1145, 1145, 1145,\n",
            "         167,  167,  167,  167,   13,   13,   13,   13,    0,    0,    0,    0,\n",
            "           8,    8,    8,    8,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           6,    6,    6,    6,  193,  193,  193,  193,   60,   60,   60,   60,\n",
            "         316,  316,  316,  316,   20,   20,   20,   20,  453,  453,  453,  453,\n",
            "        6469,    0,    0,    0,    0,    0,    0,    0, 1145, 1145, 1145, 1145,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,   15,   15,   15,   15,\n",
            "          26,   26,   26,   26, 3577, 3577, 3577, 3577,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,  193,  193,  193,  193,   20,   20,   20,   20,\n",
            "        7644, 7644, 7644, 7644, 1016, 1016, 1016, 1016,    0,    0,    0,    0,\n",
            "         187,  187,  187,  187, 2784,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,   26,   26,   26,   26,    0,    0,    0,    0,\n",
            "         933,  933,  933,  933, 1454, 1454, 1454, 1454,   68,   68,   68,   68,\n",
            "           2,    0,    0,    0,    0,    0,    0,    0,   20,   20,   20,   20,\n",
            "           0,    0,    0,    0,  545,  545,  545,  545,    8,    8,    8,    8,\n",
            "        1320, 1320, 1320, 1320,    3,    3,    3,    3,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,   26,   26,   26,   26, 1454, 1454, 1454, 1454,\n",
            "           0,    0,    0,    0,  453,  453,  453,  453,    4,    4,    4,    4,\n",
            "        1391, 1391, 1391, 1391,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0, 1320, 1320, 1320, 1320,  256,  256,  256,  256,\n",
            "        3577, 3577, 3577, 3577,  286,  286,  286,  286,    2,    2,    2,    2,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0, 1454, 1454, 1454, 1454,\n",
            "           4,    4,    4,    4,  276,  276,  276,  276,  187,  187,  187,  187,\n",
            "         377,  377,  377,  377,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0, 1320, 1320, 1320, 1320,  286,  286,  286,  286,\n",
            "           0,    0,    0,    0,   68,   68,   68,   68,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           4,    4,    4,    4,  377,  377,  377,  377,   64,   64,   64,   64,\n",
            "           3,    3,    3,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,  286,  286,  286,  286,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0, 1391, 1391, 1391, 1391,\n",
            "          14,   14,   14,   14,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,  377,  377,  377,  377,    0,    0,    0,    0,\n",
            "          46,   46,   46,   46,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,   14,   14,   14,   14,   34,   34,   34,   34,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,   14,   14,   14,   14,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           2,    2,    2,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0], device='cuda:0'), Shape: torch.Size([896])\n",
            "\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 10003 while Y.shape[1] == 1",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[41], line 35\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget: \u001b[39m\u001b[39m{\u001b[39;00mtrg\u001b[39m}\u001b[39;00m\u001b[39m, Shape: \u001b[39m\u001b[39m{\u001b[39;00mtrg\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m trg_embed \u001b[39m=\u001b[39m trg\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output_embed\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m---> 35\u001b[0m cosine_similarities \u001b[39m=\u001b[39m cosine_similarity(output_embed, trg_embed\u001b[39m.\u001b[39;49mT)\n\u001b[0;32m     36\u001b[0m mean_cosine_similarity \u001b[39m=\u001b[39m cosine_similarities\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     38\u001b[0m total_cosine_similarity \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m mean_cosine_similarity\n",
            "File \u001b[1;32mc:\\Users\\fruda\\anaconda3\\envs\\AI-Lab\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1393\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1359\u001b[0m \n\u001b[0;32m   1360\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[39m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1393\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[0;32m   1395\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1396\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
            "File \u001b[1;32mc:\\Users\\fruda\\anaconda3\\envs\\AI-Lab\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:180\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    175\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPrecomputed metric requires shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(n_queries, n_indexed). Got (\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfor \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m indexed.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m    178\u001b[0m         )\n\u001b[0;32m    179\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[1;32m--> 180\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mX.shape[1] == \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m while Y.shape[1] == \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m X, Y\n",
            "\u001b[1;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 10003 while Y.shape[1] == 1"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "total_cosine_similarity = 0\n",
        "total_examples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (images, prompts, trg_lengths) in enumerate(test_loader):\n",
        "        images = images.to(device)\n",
        "        prompts = prompts.to(device)\n",
        "\n",
        "        output = model(images, prompts)\n",
        "        print(f\"Output Shape: {output.shape}\")\n",
        "\n",
        "        # Remove the <sos> token and reshape the output and target tensors\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[1:].view(-1, output_dim).contiguous()\n",
        "        print(f\"Output Shape -1: {output.shape[-1]}\")\n",
        "        print(f\"Output View Shape: {output[1:].view(-1, output_dim).shape}\")\n",
        "        print(f\"Output View Shape: {output[1:].view(-1, output_dim).contiguous().shape}\")\n",
        "        trg = prompts.transpose(0, 1)[1:].contiguous().view(-1)\n",
        "        print(f\"Prompts Shape: {prompts.shape}\")\n",
        "        print(f\"Prompts Transpose and remove sos Shape: {prompts.transpose(0, 1)[1:].shape}\")\n",
        "        print(f\"Prompts Transpose and remove sos and contiguous Shape: {prompts.transpose(0, 1)[1:].contiguous().shape}\")\n",
        "        print(f\"Prompts Transpose and remove sos and contiguous and view -1 Shape: {prompts.transpose(0, 1)[1:].contiguous().view(-1).shape}\")\n",
        "\n",
        "        print(f\"Output: {output}, Shape: {output.shape}\\n\")\n",
        "        output_embed = output.detach().cpu().numpy().reshape(-1, output_dim)\n",
        "        print(f\"Target: {trg}, Shape: {trg.shape}\\n\")\n",
        "        trg_embed = trg.detach().cpu().numpy().reshape(-1, output_embed.shape[0])\n",
        "\n",
        "\n",
        "\n",
        "        cosine_similarities = cosine_similarity(output_embed, trg_embed.T)\n",
        "        mean_cosine_similarity = cosine_similarities.mean()\n",
        "\n",
        "        total_cosine_similarity += mean_cosine_similarity\n",
        "        total_examples += output_embed.shape[0]\n",
        "\n",
        "        if i % batch_size == 0:\n",
        "            print('Test Step [{}/{}], Mean Cosine Similarity: {:5.4f}'\n",
        "                  .format(i, len(test_loader), mean_cosine_similarity))\n",
        "\n",
        "average_cosine_similarity = total_cosine_similarity / total_examples\n",
        "print('Average Mean Cosine Similarity: {:5.4f}'.format(average_cosine_similarity))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "039f546913984c7ca4de05d891370828": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4475e962b705495195340471881ea91c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d42d7d8daac427db6a054141236e03d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79b58f471c4342feb83e107166be8651": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d77e6f4b38c41f0af41b07b0be3c107": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f75e2f36640c4154b7b2363a57143b9d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9eba77a7857a4310b4bfe37403d3736e",
            "value": 1
          }
        },
        "9eba77a7857a4310b4bfe37403d3736e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0941e17d962401f9514f3d87df0a65f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e311b0f9685146958444dcdc7870d099",
              "IPY_MODEL_8d77e6f4b38c41f0af41b07b0be3c107",
              "IPY_MODEL_fd3d00547fe54bcda39f22a6b18b7fc4"
            ],
            "layout": "IPY_MODEL_4475e962b705495195340471881ea91c"
          }
        },
        "e311b0f9685146958444dcdc7870d099": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f31cea9b62204578a45548a972364780",
            "placeholder": "​",
            "style": "IPY_MODEL_039f546913984c7ca4de05d891370828",
            "value": "100%"
          }
        },
        "f31cea9b62204578a45548a972364780": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f75e2f36640c4154b7b2363a57143b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd3d00547fe54bcda39f22a6b18b7fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79b58f471c4342feb83e107166be8651",
            "placeholder": "​",
            "style": "IPY_MODEL_5d42d7d8daac427db6a054141236e03d",
            "value": " 1/1 [00:00&lt;00:00, 38.47it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
